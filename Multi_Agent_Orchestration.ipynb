{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fLBfyd9CvvwO",
        "outputId": "619c9810-6299-4d89-c50c-06c713937799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.9-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.72)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.7)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (0.4.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.42)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.9-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain-google-genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.18 langchain-google-genai-2.1.9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "8b15d0a227f94399b7b7b0a426c6eeed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (4.13.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.8.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (4.14.1)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=78912c5bcc1ef721b705309a351d4ba9d2b68696f613547ac3652d6f10093b53\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/ab/cb/45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "# Instead of openai-agents, you would use libraries like these:\n",
        "!pip install langchain-google-genai langchain-core langchain\n",
        "!pip install wikipedia"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checks"
      ],
      "metadata": {
        "id": "0-PWrhxQ0R-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "azhWm1PJv629"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
      ],
      "metadata": {
        "id": "fDBp7i0dwknu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guardrail & Triage Routing"
      ],
      "metadata": {
        "id": "HjkKKIdl0UHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain langchain-google-genai wikipedia pydantic\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "import textwrap\n",
        "from typing import Literal\n",
        "\n",
        "# Use pydantic's standard BaseModel\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain_core.tools import tool\n",
        "import wikipedia\n",
        "from google.colab import userdata\n",
        "\n",
        "# --- SETUP: DUMMY HR POLICY FILE ---\n",
        "# This creates a fake HR policy file for the HR Agent to use.\n",
        "hr_policy_content = \"\"\"\n",
        "# Company Leave Policy\n",
        "\n",
        "## General Provisions\n",
        "Full-time staff are entitled to 24 days of annual leave per year.\n",
        "This accrues at a rate of 2 days per month.\n",
        "Up to 5 unused days may be carried forward into the next year with manager approval.\n",
        "\n",
        "## Sick Leave\n",
        "Sick leave is determined by service duration.\n",
        "- Up to 1 year of service: 2 weeks full pay, 2 weeks half pay.\n",
        "- 1-3 years of service: 4 weeks full pay, 4 weeks half pay.\n",
        "- Over 3 years of service: 8 weeks full pay, 8 weeks half pay.\n",
        "A doctor's note is required for absences longer than 3 consecutive days.\n",
        "\n",
        "## Special Leave\n",
        "- Compassionate Leave: Up to 5 days per year for the loss of a close family member.\n",
        "- Public Holidays: The company observes all official public holidays in the country of employment.\n",
        "\"\"\"\n",
        "with open(\"hr_policy.txt\", \"w\") as f:\n",
        "    f.write(hr_policy_content)\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "# 2. Configure API Key and Initialize the LLM\n",
        "# -------------------------------------------\n",
        "llm = None  # Initialize llm to None\n",
        "try:\n",
        "    os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "    llm.invoke(\"Test query to validate API key\")\n",
        "    print(\"‚úÖ Gemini API Key configured successfully.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error configuring Gemini API. Please ensure your key is correct. Details: {e}\")\n",
        "    # Exit if the LLM can't be configured\n",
        "    exit()\n",
        "\n",
        "\n",
        "# 3. Define Tools for the Agents\n",
        "# ------------------------------\n",
        "@tool\n",
        "def wiki_tool(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Fetches a summary of the given query from Wikipedia.\n",
        "    Use this for technical questions about concepts, people, or places.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        summary = wikipedia.summary(query, auto_suggest=False, sentences=5)\n",
        "        return summary\n",
        "    except wikipedia.exceptions.PageError:\n",
        "        return f\"Could not find a Wikipedia page for '{query}'. Please try a different query.\"\n",
        "    except wikipedia.exceptions.DisambiguationError as e:\n",
        "        return f\"'{query}' is ambiguous. It could refer to: {e.options[:5]}. Please be more specific.\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred while fetching from Wikipedia: {str(e)}\"\n",
        "\n",
        "@tool\n",
        "def file_search_tool(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Searches the content of the company's HR policy document.\n",
        "    Use this for any questions about leave, benefits, or other HR-related policies.\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîé Searching HR policy for: '{query}'...\")\n",
        "    with open(\"hr_policy.txt\", \"r\") as f:\n",
        "        policy_content = f.read()\n",
        "    return policy_content\n",
        "\n",
        "\n",
        "# 4. Define All Agents\n",
        "# --------------------\n",
        "\n",
        "# Agent 4a: Guardrail Agent (for input validation)\n",
        "class GuardrailOutput(BaseModel):\n",
        "    \"\"\"\n",
        "    Schema for the guardrail's decision.\n",
        "    `is_valid` is true if the query is related to HR or technical topics.\n",
        "    \"\"\"\n",
        "    is_valid: bool = Field(description=\"True if the user's query is about HR or a technical topic, False otherwise.\")\n",
        "    reasoning: str = Field(description=\"A brief, one-sentence explanation for the decision.\")\n",
        "\n",
        "guardrail_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are a gatekeeper. Your job is to determine if a user's query is on-topic.\n",
        "    The allowed topics are:\n",
        "    1.  Human Resources (HR) and workplace policies (e.g., leave, benefits).\n",
        "    2.  Technical subjects (e.g., AI, programming, science, software).\n",
        "\n",
        "    Any other topic, such as entertainment, sports, or general chit-chat, is considered off-topic.\n",
        "    You must respond only with the structured output.\"\"\"),\n",
        "    (\"human\", \"User Query: {query}\")\n",
        "])\n",
        "\n",
        "guardrail_agent_runnable = guardrail_prompt | llm.with_structured_output(GuardrailOutput)\n",
        "\n",
        "\n",
        "# Agent 4b: H.R. Agent\n",
        "hr_agent_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful H.R. assistant. You must use the `file_search_tool` to find information in the company policy document to answer the user's question. Answer based ONLY on the information retrieved from the tool.\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "])\n",
        "hr_tools = [file_search_tool]\n",
        "hr_agent = create_tool_calling_agent(llm, hr_tools, hr_agent_prompt)\n",
        "hr_agent_executor = AgentExecutor(agent=hr_agent, tools=hr_tools, verbose=False)\n",
        "\n",
        "\n",
        "# Agent 4c: Technical Agent\n",
        "tech_agent_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful technical assistant. You must use the `wiki_tool` to answer the user's question. Provide a clear and concise explanation based on the information from the tool.\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "])\n",
        "tech_tools = [wiki_tool]\n",
        "tech_agent = create_tool_calling_agent(llm, tech_tools, tech_agent_prompt)\n",
        "tech_agent_executor = AgentExecutor(agent=tech_agent, tools=tech_tools, verbose=False)\n",
        "\n",
        "\n",
        "# 5. Define the Triage Agent (The Router)\n",
        "# ---------------------------------------\n",
        "class TriageDecision(BaseModel):\n",
        "    \"\"\"The decision on which specialist agent to route the user's query to.\"\"\"\n",
        "    agent: Literal[\"HR\", \"Technical\"] = Field(description=\"The name of the agent to route to, either 'HR' or 'Technical'.\")\n",
        "\n",
        "triage_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are a triage agent. Your job is to analyze the user's query and decide which specialist is best suited to handle it. Do not answer the question yourself.\n",
        "\n",
        "    - If the query is about company policies, benefits, leave, or other workplace matters, route to 'HR'.\n",
        "    - If the query is about a concept, technology, or any general knowledge topic, route to 'Technical'.\n",
        "\n",
        "    You must respond only with the structured output.\"\"\"),\n",
        "    (\"human\", \"User Query: {query}\")\n",
        "])\n",
        "\n",
        "triage_agent_runnable = triage_prompt | llm.with_structured_output(TriageDecision)\n",
        "\n",
        "\n",
        "# 6. Orchestrate the System\n",
        "# -------------------------\n",
        "async def run_agent_system(query: str):\n",
        "    \"\"\"\n",
        "    Main function to run the multi-agent system.\n",
        "    1. Checks if the query is valid using the Guardrail.\n",
        "    2. If valid, uses the Triage agent to select the correct specialist.\n",
        "    3. Invokes the selected specialist agent to get the final answer.\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(f\"üë§ User Query: {query}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # --- Step 1: Guardrail Check ---\n",
        "    print(\"üõ°Ô∏è  Running Guardrail Check...\")\n",
        "    guardrail_result = await guardrail_agent_runnable.ainvoke({\"query\": query})\n",
        "\n",
        "    if not guardrail_result.is_valid:\n",
        "        print(f\"‚ùå Guardrail Blocked Input. Reason: {guardrail_result.reasoning}\\n\")\n",
        "        return\n",
        "\n",
        "    print(f\"‚úÖ Guardrail Passed. Reason: {guardrail_result.reasoning}\")\n",
        "\n",
        "    # --- Step 2: Triage Routing ---\n",
        "    print(\"\\nüö¶ Running Triage Agent to select specialist...\")\n",
        "    triage_decision = await triage_agent_runnable.ainvoke({\"query\": query})\n",
        "    selected_agent = triage_decision.agent\n",
        "    print(f\"üéØ Specialist selected: {selected_agent}\")\n",
        "\n",
        "    # --- Step 3: Invoke Specialist Agent ---\n",
        "    print(f\"\\n‚ñ∂Ô∏è  Invoking {selected_agent} Agent...\")\n",
        "    if selected_agent == \"HR\":\n",
        "        result = await hr_agent_executor.ainvoke({\"input\": query})\n",
        "    elif selected_agent == \"Technical\":\n",
        "        result = await tech_agent_executor.ainvoke({\"input\": query})\n",
        "    else:\n",
        "        result = {\"output\": \"Error: Triage agent selected an unknown specialist.\"}\n",
        "\n",
        "    final_answer = result.get('output', 'Sorry, I could not process your request.')\n",
        "\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"ü§ñ Final Answer:\")\n",
        "    print(textwrap.fill(final_answer, width=80))\n",
        "    print(\"-\"*60 + \"\\n\")\n",
        "\n",
        "\n",
        "# 7. Run Example Queries\n",
        "# ----------------------\n",
        "async def main():\n",
        "    # Example 1: A technical question (should be routed to Technical Agent)\n",
        "    await run_agent_system(\"What is Agentic AI?\")\n",
        "\n",
        "    # Example 2: An HR question (should be routed to HR Agent)\n",
        "    await run_agent_system(\"How many sick days do I get after working here for two years?\")\n",
        "\n",
        "    # Example 3: An out-of-scope question (should be blocked by Guardrail)\n",
        "    await run_agent_system(\"Suggest some good bollywood movies from the 1990s.\")\n",
        "\n",
        "\n",
        "if llm: # Only run if the LLM was successfully initialized\n",
        "    await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeyeWOAhxYJl",
        "outputId": "1ff0c4d1-60c4-4ac3-97f3-c162d77817bc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Gemini API Key configured successfully.\n",
            "============================================================\n",
            "üë§ User Query: What is Agentic AI?\n",
            "============================================================\n",
            "üõ°Ô∏è  Running Guardrail Check...\n",
            "‚úÖ Guardrail Passed. Reason: The query is about a technical topic (AI).\n",
            "\n",
            "üö¶ Running Triage Agent to select specialist...\n",
            "üéØ Specialist selected: Technical\n",
            "\n",
            "‚ñ∂Ô∏è  Invoking Technical Agent...\n",
            "\n",
            "------------------------------------------------------------\n",
            "ü§ñ Final Answer:\n",
            "Agentic AI refers to artificial intelligence systems designed to operate\n",
            "autonomously, making decisions and performing tasks without direct human\n",
            "intervention.  These systems react independently to conditions to produce\n",
            "results.  It's closely related to agentic automation (or agent-based process\n",
            "management systems) when applied to process automation.  Applications span\n",
            "various fields including software development, customer support, cybersecurity,\n",
            "and business intelligence.\n",
            "------------------------------------------------------------\n",
            "\n",
            "============================================================\n",
            "üë§ User Query: How many sick days do I get after working here for two years?\n",
            "============================================================\n",
            "üõ°Ô∏è  Running Guardrail Check...\n",
            "‚úÖ Guardrail Passed. Reason: The query is about HR policies, specifically about sick leave.\n",
            "\n",
            "üö¶ Running Triage Agent to select specialist...\n",
            "üéØ Specialist selected: HR\n",
            "\n",
            "‚ñ∂Ô∏è  Invoking HR Agent...\n",
            "\n",
            "------------------------------------------------------------\n",
            "ü§ñ Final Answer:\n",
            "I'm sorry, I don't have enough information to answer. I need to access the\n",
            "company's HR policy document to find the answer.  Can you provide me with access\n",
            "to that document?\n",
            "------------------------------------------------------------\n",
            "\n",
            "============================================================\n",
            "üë§ User Query: Suggest some good bollywood movies from the 1990s.\n",
            "============================================================\n",
            "üõ°Ô∏è  Running Guardrail Check...\n",
            "‚ùå Guardrail Blocked Input. Reason: The query is about Bollywood movies, which is an off-topic entertainment subject.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integrating with RAG"
      ],
      "metadata": {
        "id": "pzZexcn90kwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-google-genai wikipedia pydantic\n",
        "!pip install langchain-community faiss-cpu # Add these for RAG"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4gdEZC-04bs",
        "outputId": "1ed475bb-91ce-4ad2-b462-41fbcc944f0c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.1.9)\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.11.7)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.72)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.12)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.42)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.18 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.6.18)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (4.13.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.4.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (2.7)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.72)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.42)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.12)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, faiss-cpu, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 faiss-cpu-1.11.0.post1 httpx-sse-0.4.1 langchain-community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import asyncio\n",
        "import textwrap\n",
        "from typing import Literal\n",
        "from google.colab import userdata\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain_core.tools import tool\n",
        "import wikipedia\n",
        "\n",
        "# --- RAG Specific Imports ---\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# --- SETUP: EXPANDED DUMMY HR POLICY FILE ---\n",
        "# We'll add more distinct sections to make the RAG process more meaningful.\n",
        "hr_policy_content = \"\"\"\n",
        "# Company Leave Policy\n",
        "\n",
        "## Annual Leave\n",
        "Full-time staff are entitled to 24 days of annual leave per year. This accrues at a rate of 2 days per month. Up to 5 unused days may be carried forward into the next year with manager approval. Requests for leave must be submitted through the HR portal at least two weeks in advance.\n",
        "\n",
        "## Sick Leave\n",
        "Sick leave is for personal illness or injury. It is determined by service duration.\n",
        "- Up to 1 year of service: 2 weeks full pay, 2 weeks half pay.\n",
        "- 1-3 years of service: 4 weeks full pay, 4 weeks half pay.\n",
        "- Over 3 years of service: 8 weeks full pay, 8 weeks half pay.\n",
        "A doctor's note is required for absences longer than 3 consecutive days.\n",
        "\n",
        "## Compassionate Leave\n",
        "Special leave for bereavement is available. Employees can take up to 5 days of paid compassionate leave per year for the loss of a close family member (spouse, child, parent, sibling).\n",
        "\n",
        "## Public Holidays\n",
        "The company observes all official public holidays in the country of employment. These days are paid leave and do not count against an employee's annual leave balance.\n",
        "\n",
        "## Work From Home (WFH) Policy\n",
        "Employees may work from home up to 2 days per week with manager approval. WFH arrangements are based on job role and performance. The company provides a stipend for home office setup. All company IT and security policies must be adhered to while working remotely.\n",
        "\"\"\"\n",
        "with open(\"hr_policy.txt\", \"w\") as f:\n",
        "    f.write(hr_policy_content)\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "# 2. Configure API Key and Initialize the LLM\n",
        "# -------------------------------------------\n",
        "llm = None\n",
        "try:\n",
        "    os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "    llm.invoke(\"Test query\")\n",
        "    print(\"‚úÖ Gemini API Key configured successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error configuring Gemini API: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# 3. NEW: Setup RAG Pipeline for HR Agent\n",
        "# -----------------------------------------\n",
        "retriever = None\n",
        "try:\n",
        "    print(\"\\n‚öôÔ∏è  Setting up RAG pipeline for HR Agent...\")\n",
        "    # a. Load the documents\n",
        "    loader = TextLoader(\"./hr_policy.txt\")\n",
        "    documents = loader.load()\n",
        "\n",
        "    # b. Split documents into smaller chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "    print(f\"üìÑ Split policy into {len(docs)} chunks.\")\n",
        "\n",
        "    # c. Create embeddings and store in a FAISS vector store\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "    vector_store = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "    # d. Create a retriever from the vector store\n",
        "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 2}) # Retrieve top 2 most relevant chunks\n",
        "    print(\"‚úÖ RAG pipeline setup complete.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error setting up RAG pipeline: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# 4. Define Tools for the Agents\n",
        "# ------------------------------\n",
        "@tool\n",
        "def wiki_tool(query: str) -> str:\n",
        "    \"\"\"Fetches a summary of the given query from Wikipedia.\"\"\"\n",
        "    try:\n",
        "        summary = wikipedia.summary(query, auto_suggest=False, sentences=5)\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred while fetching from Wikipedia: {str(e)}\"\n",
        "\n",
        "@tool\n",
        "def hr_rag_tool(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Searches the HR knowledge base for policies on leave, benefits, and workplace rules.\n",
        "    Use this to answer any HR-related questions.\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîé Retrieving HR context for: '{query}'...\")\n",
        "    docs = retriever.invoke(query)\n",
        "    # Format the retrieved documents into a single string\n",
        "    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "    return f\"Retrieved context:\\n{context}\"\n",
        "\n",
        "\n",
        "# 5. Define All Agents (Guardrail, HR, Technical, Triage)\n",
        "# --------------------------------------------------------\n",
        "# Guardrail, Technical, and Triage agents remain unchanged.\n",
        "# We only need to update the HR agent to use the new RAG tool.\n",
        "\n",
        "# Agent 5a: Guardrail Agent\n",
        "class GuardrailOutput(BaseModel):\n",
        "    is_valid: bool = Field(description=\"True if query is about HR/tech, False otherwise.\")\n",
        "    reasoning: str = Field(description=\"A brief explanation for the decision.\")\n",
        "guardrail_prompt = ChatPromptTemplate.from_messages([(\"system\", \"You are a gatekeeper. Your job is to determine if a user's query is on-topic (HR or Technical). Respond only with the structured output.\"), (\"human\", \"User Query: {query}\")])\n",
        "guardrail_agent_runnable = guardrail_prompt | llm.with_structured_output(GuardrailOutput)\n",
        "\n",
        "# Agent 5b: H.R. Agent (Now powered by RAG)\n",
        "hr_agent_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful H.R. assistant. You must use the `hr_rag_tool` to find information in the company policy document. Answer the user's question based ONLY on the retrieved context from the tool.\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "])\n",
        "hr_tools = [hr_rag_tool] # Use the new RAG tool\n",
        "hr_agent = create_tool_calling_agent(llm, hr_tools, hr_agent_prompt)\n",
        "hr_agent_executor = AgentExecutor(agent=hr_agent, tools=hr_tools, verbose=False)\n",
        "\n",
        "# Agent 5c: Technical Agent\n",
        "tech_agent_prompt = ChatPromptTemplate.from_messages([(\"system\", \"You are a technical assistant. Use the `wiki_tool` to answer the user's question.\"), (\"human\", \"{input}\"), (\"placeholder\", \"{agent_scratchpad}\")])\n",
        "tech_tools = [wiki_tool]\n",
        "tech_agent = create_tool_calling_agent(llm, tech_tools, tech_agent_prompt)\n",
        "tech_agent_executor = AgentExecutor(agent=tech_agent, tools=tech_tools, verbose=False)\n",
        "\n",
        "# Agent 5d: Triage Agent\n",
        "class TriageDecision(BaseModel):\n",
        "    agent: Literal[\"HR\", \"Technical\"]\n",
        "triage_prompt = ChatPromptTemplate.from_messages([(\"system\", \"You are a triage agent. Analyze the user's query and decide if it should be handled by 'HR' or 'Technical'. Do not answer the question. Respond only with the structured output.\"), (\"human\", \"User Query: {query}\")])\n",
        "triage_agent_runnable = triage_prompt | llm.with_structured_output(TriageDecision)\n",
        "\n",
        "\n",
        "# 6. Orchestrate the System\n",
        "# -------------------------\n",
        "async def run_agent_system(query: str):\n",
        "    print(\"=\"*60)\n",
        "    print(f\"üë§ User Query: {query}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"üõ°Ô∏è  Running Guardrail Check...\")\n",
        "    guardrail_result = await guardrail_agent_runnable.ainvoke({\"query\": query})\n",
        "    if not guardrail_result.is_valid:\n",
        "        print(f\"‚ùå Guardrail Blocked Input. Reason: {guardrail_result.reasoning}\\n\")\n",
        "        return\n",
        "    print(f\"‚úÖ Guardrail Passed. Reason: {guardrail_result.reasoning}\")\n",
        "\n",
        "    print(\"\\nüö¶ Running Triage Agent to select specialist...\")\n",
        "    triage_decision = await triage_agent_runnable.ainvoke({\"query\": query})\n",
        "    selected_agent = triage_decision.agent\n",
        "    print(f\"üéØ Specialist selected: {selected_agent}\")\n",
        "\n",
        "    print(f\"\\n‚ñ∂Ô∏è  Invoking {selected_agent} Agent...\")\n",
        "    if selected_agent == \"HR\":\n",
        "        result = await hr_agent_executor.ainvoke({\"input\": query})\n",
        "    else:\n",
        "        result = await tech_agent_executor.ainvoke({\"input\": query})\n",
        "\n",
        "    final_answer = result.get('output', 'Sorry, I could not process your request.')\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"ü§ñ Final Answer:\")\n",
        "    print(textwrap.fill(final_answer, width=80))\n",
        "    print(\"-\"*60 + \"\\n\")\n",
        "\n",
        "\n",
        "# 7. Run Example Queries\n",
        "# ----------------------\n",
        "async def main():\n",
        "    # Example 1: Technical question\n",
        "    await run_agent_system(\"What is the difference between AI, Machine Learning, and Deep Learning?\")\n",
        "\n",
        "    # Example 2: HR question that requires specific retrieval via RAG\n",
        "    await run_agent_system(\"What is the policy for working from home and do I get any money for setup?\")\n",
        "\n",
        "    # Example 3: Another HR question\n",
        "    await run_agent_system(\"I need to take time off because my father passed away. What should I do?\")\n",
        "\n",
        "    # Example 4: Out-of-scope question\n",
        "    await run_agent_system(\"Can you recommend a good place for lunch in Mumbai?\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if llm and retriever: # Only run if LLM and RAG retriever were initialized\n",
        "        # In a Jupyter/Colab notebook, await the main function directly\n",
        "        # await main()\n",
        "        # In a standard .py script, use asyncio.run()\n",
        "        try:\n",
        "             asyncio.run(main())\n",
        "        except RuntimeError: # Handles the case of running in a notebook\n",
        "             await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuOzIXN2yFVf",
        "outputId": "2f1d8a15-6088-4614-defb-0f2acefbe2f5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Gemini API Key configured successfully.\n",
            "\n",
            "‚öôÔ∏è  Setting up RAG pipeline for HR Agent...\n",
            "üìÑ Split policy into 2 chunks.\n",
            "‚úÖ RAG pipeline setup complete.\n",
            "============================================================\n",
            "üë§ User Query: What is the difference between AI, Machine Learning, and Deep Learning?\n",
            "============================================================\n",
            "üõ°Ô∏è  Running Guardrail Check...\n",
            "‚úÖ Guardrail Passed. Reason: Query is about a technical topic (AI, ML, DL).\n",
            "\n",
            "üö¶ Running Triage Agent to select specialist...\n",
            "üéØ Specialist selected: Technical\n",
            "\n",
            "‚ñ∂Ô∏è  Invoking Technical Agent...\n",
            "\n",
            "------------------------------------------------------------\n",
            "ü§ñ Final Answer:\n",
            "I am sorry, I could not find a Wikipedia page that directly addresses the\n",
            "difference between AI, Machine Learning, and Deep Learning.  However, I can\n",
            "explain the relationship between these concepts.  Artificial Intelligence (AI)\n",
            "is the broadest concept, encompassing any technique that enables computers to\n",
            "mimic human intelligence.  This includes a wide range of approaches, from simple\n",
            "rule-based systems to complex algorithms.  Machine Learning (ML) is a subset of\n",
            "AI. It focuses on enabling systems to learn from data without being explicitly\n",
            "programmed.  Instead of relying on pre-defined rules, ML algorithms identify\n",
            "patterns and make predictions based on the data they are trained on.  Deep\n",
            "Learning (DL) is a subset of ML. It uses artificial neural networks with\n",
            "multiple layers (hence \"deep\") to analyze data and extract complex features.\n",
            "Deep learning is particularly effective for tasks involving large amounts of\n",
            "unstructured data, such as images, audio, and text.  In short:  AI is the\n",
            "overarching field, ML is a specific approach within AI, and DL is a specialized\n",
            "technique within ML.  Deep learning is a more powerful but also more\n",
            "computationally expensive approach than other machine learning techniques.\n",
            "------------------------------------------------------------\n",
            "\n",
            "============================================================\n",
            "üë§ User Query: What is the policy for working from home and do I get any money for setup?\n",
            "============================================================\n",
            "üõ°Ô∏è  Running Guardrail Check...\n",
            "‚úÖ Guardrail Passed. Reason: Query is about HR policy.\n",
            "\n",
            "üö¶ Running Triage Agent to select specialist...\n",
            "üéØ Specialist selected: HR\n",
            "\n",
            "‚ñ∂Ô∏è  Invoking HR Agent...\n",
            "\n",
            "üîé Retrieving HR context for: 'working from home policy'...\n",
            "\n",
            "------------------------------------------------------------\n",
            "ü§ñ Final Answer:\n",
            "Based on the company policy, employees may work from home up to 2 days a week\n",
            "with manager approval.  This is dependent on your job role and performance. The\n",
            "policy also states that the company provides a stipend for home office setup.\n",
            "You should adhere to all company IT and security policies while working\n",
            "remotely.\n",
            "------------------------------------------------------------\n",
            "\n",
            "============================================================\n",
            "üë§ User Query: I need to take time off because my father passed away. What should I do?\n",
            "============================================================\n",
            "üõ°Ô∏è  Running Guardrail Check...\n",
            "‚úÖ Guardrail Passed. Reason: Query is about HR.\n",
            "\n",
            "üö¶ Running Triage Agent to select specialist...\n",
            "üéØ Specialist selected: HR\n",
            "\n",
            "‚ñ∂Ô∏è  Invoking HR Agent...\n",
            "\n",
            "üîé Retrieving HR context for: 'policy on bereavement leave'...\n",
            "\n",
            "------------------------------------------------------------\n",
            "ü§ñ Final Answer:\n",
            "Based on the company policy, you are entitled to up to 5 days of paid\n",
            "compassionate leave for the loss of a close family member, which includes\n",
            "parents.  Please submit your leave request through the HR portal.\n",
            "------------------------------------------------------------\n",
            "\n",
            "============================================================\n",
            "üë§ User Query: Can you recommend a good place for lunch in Mumbai?\n",
            "============================================================\n",
            "üõ°Ô∏è  Running Guardrail Check...\n",
            "‚ùå Guardrail Blocked Input. Reason: Query is not related to HR or tech.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-90055898.py:202: RuntimeWarning: coroutine 'main' was never awaited\n",
            "  await main()\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parent Document Retriever (Hierarchical RAG)\n",
        "\n",
        "- InMemoryStore to hold the parent documents.\n",
        "- child_splitter to create small, precise chunks for searching and a parent_splitter to create the larger, context-rich chunks that will be retrieved."
      ],
      "metadata": {
        "id": "OBL_t5ZGBQGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-google-genai wikipedia pydantic -q\n",
        "!pip install langchain-community faiss-cpu -q"
      ],
      "metadata": {
        "id": "mAvH1310Bx_D"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import asyncio\n",
        "import textwrap\n",
        "from typing import Literal, List\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.documents import Document\n",
        "import wikipedia\n",
        "\n",
        "# --- RAG Specific Imports ---\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# --- SETUP: HR POLICY FILE ---\n",
        "hr_policy_content = \"\"\"\n",
        "# Company Leave Policy\n",
        "\n",
        "## Annual Leave\n",
        "Full-time staff are entitled to 24 days of annual leave per year. This accrues at a rate of 2 days per month. Up to 5 unused days may be carried forward into the next year with manager approval. Requests for leave must be submitted through the HR portal at least two weeks in advance.\n",
        "\n",
        "## Sick Leave\n",
        "Sick leave is for personal illness or injury. It is determined by service duration.\n",
        "- Up to 1 year of service: 2 weeks full pay, 2 weeks half pay.\n",
        "- 1-3 years of service: 4 weeks full pay, 4 weeks half pay.\n",
        "- Over 3 years of service: 8 weeks full pay, 8 weeks half pay.\n",
        "A doctor's note is required for absences longer than 3 consecutive days.\n",
        "\n",
        "## Compassionate Leave\n",
        "Special leave for bereavement is available. Employees can take up to 5 days of paid compassionate leave per year for the loss of a close family member (spouse, child, parent, sibling).\n",
        "\n",
        "## Public Holidays\n",
        "The company observes all official public holidays in the country of employment. These days are paid leave and do not count against an employee's annual leave balance.\n",
        "\n",
        "## Work From Home (WFH) Policy\n",
        "Employees may work from home up to 2 days per week with manager approval. WFH arrangements are based on job role and performance. The company provides a stipend for home office setup. All company IT and security policies must be adhered to while working remotely.\n",
        "\"\"\"\n",
        "with open(\"hr_policy.txt\", \"w\") as f:\n",
        "    f.write(hr_policy_content)\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "# 2. Configure API Key\n",
        "# --------------------\n",
        "llm = None\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "    llm.invoke(\"Test query\")\n",
        "    print(\"‚úÖ Gemini API Key configured successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error configuring Gemini API: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# 3. NEW: Robust Parent-Child RAG Setup\n",
        "# ---------------------------------------\n",
        "retriever = None\n",
        "try:\n",
        "    print(\"\\n‚öôÔ∏è  Setting up Robust Parent-Child RAG pipeline...\")\n",
        "    # a. Load the documents\n",
        "    loader = TextLoader(\"./hr_policy.txt\")\n",
        "    docs = loader.load()\n",
        "\n",
        "    # b. Define the splitters\n",
        "    parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
        "    child_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n",
        "\n",
        "    # c. Split documents into parent chunks\n",
        "    parent_documents = parent_splitter.split_documents(docs)\n",
        "\n",
        "    # d. Create child documents from parents and store parent text in metadata\n",
        "    child_documents = []\n",
        "    for parent_doc in parent_documents:\n",
        "        child_docs = child_splitter.split_text(parent_doc.page_content)\n",
        "        for child_doc_text in child_docs:\n",
        "            # Create a new Document for each child with parent's content in metadata\n",
        "            child_documents.append(\n",
        "                Document(page_content=child_doc_text, metadata={\"parent_content\": parent_doc.page_content})\n",
        "            )\n",
        "    print(f\"üìÑ Created {len(parent_documents)} parent chunks and {len(child_documents)} child chunks for indexing.\")\n",
        "\n",
        "    # e. Create a standard, reliable FAISS vector store from the child documents\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "    vectorstore = FAISS.from_documents(child_documents, embeddings)\n",
        "\n",
        "    # f. Create a standard retriever\n",
        "    retriever = vectorstore.as_retriever()\n",
        "    print(\"‚úÖ Robust RAG pipeline setup complete.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error setting up RAG pipeline: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# 4. Define Tools for the Agents\n",
        "# ------------------------------\n",
        "@tool\n",
        "def wiki_tool(query: str) -> str:\n",
        "    \"\"\"Fetches a summary of the given query from Wikipedia.\"\"\"\n",
        "    try:\n",
        "        summary = wikipedia.summary(query, auto_suggest=False, sentences=5)\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred while fetching from Wikipedia: {str(e)}\"\n",
        "\n",
        "@tool\n",
        "def hr_rag_tool(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Searches the HR knowledge base for policies on leave, benefits, and workplace rules.\n",
        "    Use this to answer any HR-related questions.\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîé Retrieving HR context for: '{query}'...\")\n",
        "    # 1. Retrieve the small, specific child documents\n",
        "    child_docs = retriever.invoke(query)\n",
        "\n",
        "    # 2. Extract the parent content from the metadata of the retrieved child documents\n",
        "    # Use a set to avoid returning duplicate parent documents\n",
        "    parent_contents = {doc.metadata['parent_content'] for doc in child_docs}\n",
        "\n",
        "    # 3. Format the unique parent documents as the context\n",
        "    context = \"\\n\\n\".join(parent_contents)\n",
        "    return f\"Retrieved context:\\n{context}\"\n",
        "\n",
        "\n",
        "# 5. Define All Agents (No changes here)\n",
        "# ----------------------------------------\n",
        "class GuardrailOutput(BaseModel):\n",
        "    is_valid: bool = Field(description=\"True if query is about HR/tech, False otherwise.\")\n",
        "    reasoning: str = Field(description=\"A brief explanation for the decision.\")\n",
        "guardrail_prompt = ChatPromptTemplate.from_messages([(\"system\", \"You are a gatekeeper. Your job is to determine if a user's query is on-topic (HR or Technical). Respond only with the structured output.\"), (\"human\", \"User Query: {query}\")])\n",
        "guardrail_agent_runnable = guardrail_prompt | llm.with_structured_output(GuardrailOutput)\n",
        "\n",
        "hr_agent_prompt = ChatPromptTemplate.from_messages([(\"system\", \"You are a helpful H.R. assistant. You must use the `hr_rag_tool` to find information in the company policy document. Answer the user's question based ONLY on the retrieved context from the tool.\"), (\"human\", \"{input}\"), (\"placeholder\", \"{agent_scratchpad}\")])\n",
        "hr_tools = [hr_rag_tool]\n",
        "hr_agent = create_tool_calling_agent(llm, hr_tools, hr_agent_prompt)\n",
        "hr_agent_executor = AgentExecutor(agent=hr_agent, tools=hr_tools, verbose=False)\n",
        "\n",
        "tech_agent_prompt = ChatPromptTemplate.from_messages([(\"system\", \"You are a technical assistant. Use the `wiki_tool` to answer the user's question.\"), (\"human\", \"{input}\"), (\"placeholder\", \"{agent_scratchpad}\")])\n",
        "tech_tools = [wiki_tool]\n",
        "tech_agent = create_tool_calling_agent(llm, tech_tools, tech_agent_prompt)\n",
        "tech_agent_executor = AgentExecutor(agent=tech_agent, tools=tech_tools, verbose=False)\n",
        "\n",
        "class TriageDecision(BaseModel):\n",
        "    agent: Literal[\"HR\", \"Technical\"]\n",
        "triage_prompt = ChatPromptTemplate.from_messages([(\"system\", \"You are a triage agent. Analyze the user's query and decide if it should be handled by 'HR' or 'Technical'. Respond only with the structured output.\"), (\"human\", \"User Query: {query}\")])\n",
        "triage_agent_runnable = triage_prompt | llm.with_structured_output(TriageDecision)\n",
        "\n",
        "\n",
        "# 6. Orchestrate the System (No changes here)\n",
        "# ---------------------------------------------\n",
        "async def run_agent_system(query: str):\n",
        "    print(\"=\"*60)\n",
        "    print(f\"üë§ User Query: {query}\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"üõ°Ô∏è  Running Guardrail Check...\")\n",
        "    guardrail_result = await guardrail_agent_runnable.ainvoke({\"query\": query})\n",
        "    if not guardrail_result.is_valid:\n",
        "        print(f\"‚ùå Guardrail Blocked Input. Reason: {guardrail_result.reasoning}\\n\")\n",
        "        return\n",
        "    print(f\"‚úÖ Guardrail Passed. Reason: {guardrail_result.reasoning}\")\n",
        "    print(\"\\nüö¶ Running Triage Agent to select specialist...\")\n",
        "    triage_decision = await triage_agent_runnable.ainvoke({\"query\": query})\n",
        "    selected_agent = triage_decision.agent\n",
        "    print(f\"üéØ Specialist selected: {selected_agent}\")\n",
        "    print(f\"\\n‚ñ∂Ô∏è  Invoking {selected_agent} Agent...\")\n",
        "    if selected_agent == \"HR\":\n",
        "        result = await hr_agent_executor.ainvoke({\"input\": query})\n",
        "    else:\n",
        "        result = await tech_agent_executor.ainvoke({\"input\": query})\n",
        "    final_answer = result.get('output', 'Sorry, I could not process your request.')\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"ü§ñ Final Answer:\")\n",
        "    print(textwrap.fill(final_answer, width=80))\n",
        "    print(\"-\"*60 + \"\\n\")\n",
        "\n",
        "\n",
        "# 7. Run Example Queries\n",
        "# ----------------------\n",
        "async def main():\n",
        "    await run_agent_system(\"What is the difference between AI, Machine Learning, and Deep Learning?\")\n",
        "    await run_agent_system(\"I've been sick for 4 days. Do I need to provide any specific documentation?\")\n",
        "    await run_agent_system(\"Where can I find the best vada pav in Mumbai?\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if llm and retriever:\n",
        "        try:\n",
        "             await main() # Use this in a notebook\n",
        "        except NameError: # Fallback for .py script\n",
        "             asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FhAzbsn1MzC",
        "outputId": "97fc7e02-68e5-4c6d-e88d-169f7f6db915"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Gemini API Key configured successfully.\n",
            "\n",
            "‚öôÔ∏è  Setting up Robust Parent-Child RAG pipeline...\n",
            "üìÑ Created 1 parent chunks and 4 child chunks for indexing.\n",
            "‚úÖ Robust RAG pipeline setup complete.\n",
            "============================================================\n",
            "üë§ User Query: What is the difference between AI, Machine Learning, and Deep Learning?\n",
            "============================================================\n",
            "üõ°Ô∏è  Running Guardrail Check...\n",
            "‚úÖ Guardrail Passed. Reason: Query is about a technical topic\n",
            "\n",
            "üö¶ Running Triage Agent to select specialist...\n",
            "üéØ Specialist selected: Technical\n",
            "\n",
            "‚ñ∂Ô∏è  Invoking Technical Agent...\n",
            "\n",
            "------------------------------------------------------------\n",
            "ü§ñ Final Answer:\n",
            "I am sorry, I could not find a Wikipedia page with information on the difference\n",
            "between AI, Machine Learning, and Deep Learning.  I need more information to\n",
            "answer your question.\n",
            "------------------------------------------------------------\n",
            "\n",
            "============================================================\n",
            "üë§ User Query: I've been sick for 4 days. Do I need to provide any specific documentation?\n",
            "============================================================\n",
            "üõ°Ô∏è  Running Guardrail Check...\n",
            "‚úÖ Guardrail Passed. Reason: Query is about HR policy.\n",
            "\n",
            "üö¶ Running Triage Agent to select specialist...\n",
            "üéØ Specialist selected: HR\n",
            "\n",
            "‚ñ∂Ô∏è  Invoking HR Agent...\n",
            "\n",
            "üîé Retrieving HR context for: 'sick leave documentation'...\n",
            "\n",
            "------------------------------------------------------------\n",
            "ü§ñ Final Answer:\n",
            "Based on the company policy, a doctor's note is required for absences longer\n",
            "than 3 consecutive days.  Since you've been sick for 4 days, you will need to\n",
            "provide a doctor's note.\n",
            "------------------------------------------------------------\n",
            "\n",
            "============================================================\n",
            "üë§ User Query: Where can I find the best vada pav in Mumbai?\n",
            "============================================================\n",
            "üõ°Ô∏è  Running Guardrail Check...\n",
            "‚ùå Guardrail Blocked Input. Reason: Query is not related to HR or Technical topics.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integrateing Query Expansion into our RAG pipeline."
      ],
      "metadata": {
        "id": "q3YwJP4EDIYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import asyncio\n",
        "import textwrap\n",
        "from typing import Literal, List\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import wikipedia\n",
        "\n",
        "# --- RAG Specific Imports ---\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# --- SETUP: HR POLICY FILE ---\n",
        "hr_policy_content = \"\"\"\n",
        "# Company Leave Policy\n",
        "\n",
        "## Annual Leave\n",
        "Full-time staff are entitled to 24 days of annual leave per year. This accrues at a rate of 2 days per month. Up to 5 unused days may be carried forward into the next year with manager approval. Requests for leave must be submitted through the HR portal at least two weeks in advance.\n",
        "\n",
        "## Sick Leave\n",
        "Sick leave is for personal illness or injury. It is determined by service duration.\n",
        "- Up to 1 year of service: 2 weeks full pay, 2 weeks half pay.\n",
        "- 1-3 years of service: 4 weeks full pay, 4 weeks half pay.\n",
        "- Over 3 years of service: 8 weeks full pay, 8 weeks half pay.\n",
        "A doctor's note is required for absences longer than 3 consecutive days.\n",
        "\n",
        "## Compassionate Leave\n",
        "Special leave for bereavement is available. Employees can take up to 5 days of paid compassionate leave per year for the loss of a close family member (spouse, child, parent, sibling).\n",
        "\n",
        "## Public Holidays\n",
        "The company observes all official public holidays in the country of employment. These days are paid leave and do not count against an employee's annual leave balance.\n",
        "\n",
        "## Work From Home (WFH) Policy\n",
        "Employees may work from home up to 2 days per week with manager approval. WFH arrangements are based on job role and performance. The company provides a stipend for home office setup. All company IT and security policies must be adhered to while working remotely.\n",
        "\"\"\"\n",
        "with open(\"hr_policy.txt\", \"w\") as f:\n",
        "    f.write(hr_policy_content)\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "# 2. Configure API Key\n",
        "# --------------------\n",
        "llm = None\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0) # Temp=0 for predictable output\n",
        "    llm.invoke(\"Test query\")\n",
        "    print(\"‚úÖ Gemini API Key configured successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error configuring Gemini API: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# 3. Robust Parent-Child RAG Setup (No changes here)\n",
        "# --------------------------------------------------\n",
        "retriever = None\n",
        "try:\n",
        "    print(\"\\n‚öôÔ∏è  Setting up Robust Parent-Child RAG pipeline...\")\n",
        "    loader = TextLoader(\"./hr_policy.txt\")\n",
        "    docs = loader.load()\n",
        "    parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
        "    child_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n",
        "    parent_documents = parent_splitter.split_documents(docs)\n",
        "    child_documents = []\n",
        "    for parent_doc in parent_documents:\n",
        "        child_docs = child_splitter.split_text(parent_doc.page_content)\n",
        "        for child_doc_text in child_docs:\n",
        "            child_documents.append(\n",
        "                Document(page_content=child_doc_text, metadata={\"parent_content\": parent_doc.page_content})\n",
        "            )\n",
        "    print(f\"üìÑ Created {len(parent_documents)} parent chunks and {len(child_documents)} child chunks for indexing.\")\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "    vectorstore = FAISS.from_documents(child_documents, embeddings)\n",
        "    retriever = vectorstore.as_retriever()\n",
        "    print(\"‚úÖ Robust RAG pipeline setup complete.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error setting up RAG pipeline: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# 4. Define Tools for the Agents\n",
        "# ------------------------------\n",
        "\n",
        "# --- NEW: Query Expansion Chain ---\n",
        "# This chain will take a query and generate alternative versions of it.\n",
        "query_expansion_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are an expert at query expansion. Your task is to rewrite a given user query into 3 alternative, more detailed versions.\n",
        "The goal is to improve the recall of a vector search.\n",
        "Provide the queries on new lines. DO NOT number the questions.\n",
        "\n",
        "Original Query:\n",
        "{query}\n",
        "\n",
        "Your Expanded Queries:\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "query_expansion_chain = query_expansion_prompt | llm | StrOutputParser()\n",
        "# ------------------------------------\n",
        "\n",
        "@tool\n",
        "def wiki_tool(query: str) -> str:\n",
        "    \"\"\"Fetches a summary of the given query from Wikipedia.\"\"\"\n",
        "    try:\n",
        "        summary = wikipedia.summary(query, auto_suggest=False, sentences=5)\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred while fetching from Wikipedia: {str(e)}\"\n",
        "\n",
        "@tool\n",
        "def hr_rag_tool(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Searches the HR knowledge base for policies on leave, benefits, and workplace rules.\n",
        "    Use this to answer any HR-related questions.\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîé Original HR query: '{query}'\")\n",
        "    # 1. Generate expanded queries\n",
        "    expanded_queries_str = query_expansion_chain.invoke({\"query\": query})\n",
        "    expanded_queries = expanded_queries_str.strip().split('\\n')\n",
        "    all_queries = [query] + expanded_queries\n",
        "    print(f\"üîç Performing retrieval with expanded queries: {all_queries}\")\n",
        "\n",
        "    # 2. Retrieve documents for all queries\n",
        "    all_retrieved_docs = []\n",
        "    for q in all_queries:\n",
        "        all_retrieved_docs.extend(retriever.invoke(q))\n",
        "\n",
        "    # 3. Get the unique parent documents from the retrieved child documents\n",
        "    unique_parent_contents = {doc.metadata['parent_content'] for doc in all_retrieved_docs}\n",
        "\n",
        "    # 4. Format the unique parent documents as the context\n",
        "    context = \"\\n\\n\".join(unique_parent_contents)\n",
        "    return f\"Retrieved context:\\n{context}\"\n",
        "\n",
        "\n",
        "# 5. Define All Agents (No changes here)\n",
        "# ----------------------------------------\n",
        "class GuardrailOutput(BaseModel):\n",
        "    is_valid: bool = Field(description=\"True if query is about HR/tech, False otherwise.\")\n",
        "    reasoning: str = Field(description=\"A brief explanation for the decision.\")\n",
        "guardrail_prompt = ChatPromptTemplate.from_messages([(\"system\", \"You are a gatekeeper. Your job is to determine if a user's query is on-topic (HR or Technical). Respond only with the structured output.\"), (\"human\", \"User Query: {query}\")])\n",
        "guardrail_agent_runnable = guardrail_prompt | llm.with_structured_output(GuardrailOutput)\n",
        "\n",
        "hr_agent_prompt = ChatPromptTemplate.from_messages([(\"system\", \"You are a helpful H.R. assistant. You must use the `hr_rag_tool` to find information in the company policy document. Answer the user's question based ONLY on the retrieved context from the tool.\"), (\"human\", \"{input}\"), (\"placeholder\", \"{agent_scratchpad}\")])\n",
        "hr_tools = [hr_rag_tool]\n",
        "hr_agent = create_tool_calling_agent(llm, hr_tools, hr_agent_prompt)\n",
        "hr_agent_executor = AgentExecutor(agent=hr_agent, tools=hr_tools, verbose=False)\n",
        "\n",
        "tech_agent_prompt = ChatPromptTemplate.from_messages([(\"system\", \"You are a technical assistant. Use the `wiki_tool` to answer the user's question.\"), (\"human\", \"{input}\"), (\"placeholder\", \"{agent_scratchpad}\")])\n",
        "tech_tools = [wiki_tool]\n",
        "tech_agent = create_tool_calling_agent(llm, tech_tools, tech_agent_prompt)\n",
        "tech_agent_executor = AgentExecutor(agent=tech_agent, tools=tech_tools, verbose=False)\n",
        "\n",
        "class TriageDecision(BaseModel):\n",
        "    agent: Literal[\"HR\", \"Technical\"]\n",
        "triage_prompt = ChatPromptTemplate.from_messages([(\"system\", \"You are a triage agent. Analyze the user's query and decide if it should be handled by 'HR' or 'Technical'. Respond only with the structured output.\"), (\"human\", \"User Query: {query}\")])\n",
        "triage_agent_runnable = triage_prompt | llm.with_structured_output(TriageDecision)\n",
        "\n",
        "\n",
        "# 6. Orchestrate the System (No changes here)\n",
        "# ---------------------------------------------\n",
        "async def run_agent_system(query: str):\n",
        "    print(\"=\"*60)\n",
        "    print(f\"üë§ User Query: {query}\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"üõ°Ô∏è  Running Guardrail Check...\")\n",
        "    guardrail_result = await guardrail_agent_runnable.ainvoke({\"query\": query})\n",
        "    if not guardrail_result.is_valid:\n",
        "        print(f\"‚ùå Guardrail Blocked Input. Reason: {guardrail_result.reasoning}\\n\")\n",
        "        return\n",
        "    print(f\"‚úÖ Guardrail Passed. Reason: {guardrail_result.reasoning}\")\n",
        "    print(\"\\nüö¶ Running Triage Agent to select specialist...\")\n",
        "    triage_decision = await triage_agent_runnable.ainvoke({\"query\": query})\n",
        "    selected_agent = triage_decision.agent\n",
        "    print(f\"üéØ Specialist selected: {selected_agent}\")\n",
        "    print(f\"\\n‚ñ∂Ô∏è  Invoking {selected_agent} Agent...\")\n",
        "    if selected_agent == \"HR\":\n",
        "        result = await hr_agent_executor.ainvoke({\"input\": query})\n",
        "    else:\n",
        "        result = await tech_agent_executor.ainvoke({\"input\": query})\n",
        "    final_answer = result.get('output', 'Sorry, I could not process your request.')\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"ü§ñ Final Answer:\")\n",
        "    print(textwrap.fill(final_answer, width=80))\n",
        "    print(\"-\"*60 + \"\\n\")\n",
        "\n",
        "\n",
        "# 7. Run Example Queries\n",
        "# ----------------------\n",
        "async def main():\n",
        "    await run_agent_system(\"What is the difference between AI, Machine Learning, and Deep Learning?\")\n",
        "    await run_agent_system(\"I've been sick for 4 days. What paperwork do I need?\")\n",
        "    await run_agent_system(\"Where can I find the best vada pav in Mumbai?\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if llm and retriever:\n",
        "        try:\n",
        "             await main()\n",
        "        except (NameError, RuntimeError):\n",
        "             # This handles both cases: 'main' not defined outside of a notebook,\n",
        "             # and 'asyncio.run' error inside a notebook.\n",
        "             try:\n",
        "                 asyncio.run(main())\n",
        "             except RuntimeError:\n",
        "                 # If we are in a notebook, the above will fail, so we just await it.\n",
        "                 # This construct makes the script runnable in both .py and .ipynb\n",
        "                 pass # The 'await main()' would have already been tried."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPL-kKctBtOW",
        "outputId": "1a910a56-141a-472a-d85f-f99559eac9a8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Gemini API Key configured successfully.\n",
            "\n",
            "‚öôÔ∏è  Setting up Robust Parent-Child RAG pipeline...\n",
            "üìÑ Created 1 parent chunks and 4 child chunks for indexing.\n",
            "‚úÖ Robust RAG pipeline setup complete.\n",
            "============================================================\n",
            "üë§ User Query: What is the difference between AI, Machine Learning, and Deep Learning?\n",
            "============================================================\n",
            "üõ°Ô∏è  Running Guardrail Check...\n",
            "‚úÖ Guardrail Passed. Reason: Query is about a technical topic (AI, ML, DL).\n",
            "\n",
            "üö¶ Running Triage Agent to select specialist...\n",
            "üéØ Specialist selected: Technical\n",
            "\n",
            "‚ñ∂Ô∏è  Invoking Technical Agent...\n",
            "\n",
            "------------------------------------------------------------\n",
            "ü§ñ Final Answer:\n",
            "I am sorry, I could not find a Wikipedia page with information on the\n",
            "differences between AI, Machine Learning, and Deep Learning.  I need more\n",
            "information to answer your question.\n",
            "------------------------------------------------------------\n",
            "\n",
            "============================================================\n",
            "üë§ User Query: I've been sick for 4 days. What paperwork do I need?\n",
            "============================================================\n",
            "üõ°Ô∏è  Running Guardrail Check...\n",
            "‚úÖ Guardrail Passed. Reason: Query is about HR.\n",
            "\n",
            "üö¶ Running Triage Agent to select specialist...\n",
            "üéØ Specialist selected: HR\n",
            "\n",
            "‚ñ∂Ô∏è  Invoking HR Agent...\n",
            "\n",
            "------------------------------------------------------------\n",
            "ü§ñ Final Answer:\n",
            "I am sorry, I cannot answer this question. I do not have access to the company's\n",
            "policy on sick leave.  I need more information to assist you.\n",
            "------------------------------------------------------------\n",
            "\n",
            "============================================================\n",
            "üë§ User Query: Where can I find the best vada pav in Mumbai?\n",
            "============================================================\n",
            "üõ°Ô∏è  Running Guardrail Check...\n",
            "‚ùå Guardrail Blocked Input. Reason: Query is not related to HR or Technical topics.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dual RAG Pipelines - Tech & HR"
      ],
      "metadata": {
        "id": "I50KP0-iD51T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import asyncio\n",
        "import textwrap\n",
        "from typing import Literal, List\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import wikipedia\n",
        "\n",
        "# --- RAG Specific Imports ---\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# --- SETUP: Create Dummy Knowledge Base Files ---\n",
        "\n",
        "# HR Policy Document\n",
        "hr_policy_content = \"\"\"\n",
        "# Company Leave Policy\n",
        "...\n",
        "\"\"\"\n",
        "with open(\"hr_policy.txt\", \"w\") as f:\n",
        "    f.write(hr_policy_content)\n",
        "\n",
        "# NEW: Technical Knowledge Base Document\n",
        "tech_docs_content = \"\"\"\n",
        "# Internal Technical Documentation\n",
        "\n",
        "## Project Phoenix: Frontend\n",
        "- Repository: git.corp.example.com/phoenix/frontend-app\n",
        "- Language: TypeScript\n",
        "- Framework: React\n",
        "- Style Guide: Adhere to the 'StandardJS' style guide.\n",
        "- Description: This is the main customer-facing web application.\n",
        "\n",
        "## Project Phoenix: Backend\n",
        "- Repository: git.corp.example.com/phoenix/backend-services\n",
        "- Language: Python\n",
        "- Framework: FastAPI\n",
        "- Description: These are the microservices that power the Phoenix frontend.\n",
        "\n",
        "## Deployment Procedures\n",
        "- All deployments to production must go through the CI/CD pipeline in Jenkins.\n",
        "- Request a production deployment by creating a JIRA ticket with the 'DEVOPS' component.\n",
        "- Staging environment is open for all developers for testing. The URL is staging.phoenix.example.com.\n",
        "\n",
        "## Coding Standards\n",
        "- All Python code must be formatted with 'black'.\n",
        "- All frontend code must pass 'eslint' checks before merging.\n",
        "- API keys and secrets must never be hard-coded. They should be loaded from HashiCorp Vault.\n",
        "\"\"\"\n",
        "with open(\"tech_docs.txt\", \"w\") as f:\n",
        "    f.write(tech_docs_content)\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "# 2. Configure API Key\n",
        "# --------------------\n",
        "llm = None\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n",
        "    llm.invoke(\"Test query\")\n",
        "    print(\"‚úÖ Gemini API Key configured successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error configuring Gemini API: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# 3. Setup RAG Pipelines\n",
        "# ----------------------\n",
        "hr_retriever = None\n",
        "tech_retriever = None\n",
        "\n",
        "# Function to create a RAG retriever to avoid code duplication\n",
        "def create_rag_retriever(file_path: str, chunk_size: int, chunk_overlap: int) -> FAISS.as_retriever:\n",
        "    loader = TextLoader(file_path)\n",
        "    docs = loader.load()\n",
        "    parent_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    child_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n",
        "    parent_documents = parent_splitter.split_documents(docs)\n",
        "    child_documents = []\n",
        "    for parent_doc in parent_documents:\n",
        "        child_docs = child_splitter.split_text(parent_doc.page_content)\n",
        "        for child_doc_text in child_docs:\n",
        "            child_documents.append(\n",
        "                Document(page_content=child_doc_text, metadata={\"parent_content\": parent_doc.page_content})\n",
        "            )\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "    vectorstore = FAISS.from_documents(child_documents, embeddings)\n",
        "    return vectorstore.as_retriever()\n",
        "\n",
        "try:\n",
        "    # 3a. HR RAG Pipeline\n",
        "    print(\"\\n‚öôÔ∏è  Setting up HR RAG pipeline...\")\n",
        "    hr_retriever = create_rag_retriever(\"./hr_policy.txt\", chunk_size=1500, chunk_overlap=200)\n",
        "    print(\"‚úÖ HR RAG pipeline setup complete.\")\n",
        "\n",
        "    # 3b. Technical RAG Pipeline\n",
        "    print(\"\\n‚öôÔ∏è  Setting up Technical RAG pipeline...\")\n",
        "    tech_retriever = create_rag_retriever(\"./tech_docs.txt\", chunk_size=1200, chunk_overlap=150)\n",
        "    print(\"‚úÖ Technical RAG pipeline setup complete.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error setting up RAG pipelines: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# 4. Define Tools for the Agents\n",
        "# ------------------------------\n",
        "query_expansion_prompt = ChatPromptTemplate.from_template(\"Rewrite the user query into 3 alternative versions to improve vector search recall.\\n\\nOriginal Query:\\n{query}\\n\\nExpanded Queries:\")\n",
        "query_expansion_chain = query_expansion_prompt | llm | StrOutputParser()\n",
        "\n",
        "@tool\n",
        "def hr_rag_tool(query: str) -> str:\n",
        "    \"\"\"Searches the HR knowledge base for company policies.\"\"\"\n",
        "    print(f\"\\nüîé Original HR query: '{query}'\")\n",
        "    expanded_queries_str = query_expansion_chain.invoke({\"query\": query})\n",
        "    all_queries = [query] + expanded_queries_str.strip().split('\\n')\n",
        "    print(f\"üîç Performing retrieval with expanded HR queries: {all_queries}\")\n",
        "    all_retrieved_docs = []\n",
        "    for q in all_queries:\n",
        "        all_retrieved_docs.extend(hr_retriever.invoke(q))\n",
        "    unique_parent_contents = {doc.metadata['parent_content'] for doc in all_retrieved_docs}\n",
        "    return f\"Retrieved context:\\n\" + \"\\n\\n\".join(unique_parent_contents)\n",
        "\n",
        "# --- NEW: Tech RAG Tool ---\n",
        "@tool\n",
        "def tech_rag_tool(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Searches the internal technical knowledge base. Use this for questions about\n",
        "    company-specific projects, repositories, deployment, and coding standards.\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîé Retrieving internal tech context for: '{query}'...\")\n",
        "    docs = tech_retriever.invoke(query)\n",
        "    context = \"\\n\\n\".join(doc.metadata['parent_content'] for doc in docs)\n",
        "    return f\"Retrieved context:\\n{context}\"\n",
        "\n",
        "@tool\n",
        "def wiki_tool(query: str) -> str:\n",
        "    \"\"\"Fetches a summary from Wikipedia for general knowledge technical questions.\"\"\"\n",
        "    print(f\"\\nüîé Searching Wikipedia for: '{query}'...\")\n",
        "    try:\n",
        "        return wikipedia.summary(query, auto_suggest=False, sentences=5)\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred fetching from Wikipedia: {e}\"\n",
        "\n",
        "\n",
        "# 5. Define All Agents\n",
        "# ----------------------------------------\n",
        "class GuardrailOutput(BaseModel):\n",
        "    is_valid: bool; reasoning: str\n",
        "guardrail_prompt = ChatPromptTemplate.from_messages([(\"system\", \"Is the user's query about HR or Technical topics? Respond only with the structured output.\"), (\"human\", \"Query: {query}\")])\n",
        "guardrail_agent_runnable = guardrail_prompt | llm.with_structured_output(GuardrailOutput)\n",
        "\n",
        "# HR Agent (no changes)\n",
        "hr_agent_prompt = ChatPromptTemplate.from_messages([(\"system\", \"You are an HR assistant. Use the `hr_rag_tool` to answer questions based ONLY on the retrieved context.\"), (\"human\", \"{input}\"), (\"placeholder\", \"{agent_scratchpad}\")])\n",
        "hr_tools = [hr_rag_tool]\n",
        "hr_agent = create_tool_calling_agent(llm, hr_tools, hr_agent_prompt)\n",
        "hr_agent_executor = AgentExecutor(agent=hr_agent, tools=hr_tools, verbose=False)\n",
        "\n",
        "# --- UPDATED: Technical Agent ---\n",
        "# Now has two tools and must choose between them.\n",
        "tech_agent_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are a specialist technical assistant. You have two tools:\n",
        "    1. `tech_rag_tool`: For questions about internal company systems, code, projects like 'Phoenix', repositories, and deployment.\n",
        "    2. `wiki_tool`: For general public technical knowledge, programming concepts, and open-source technologies.\n",
        "\n",
        "    You must decide which tool is appropriate. If the question is about company specifics, use `tech_rag_tool`. Otherwise, use `wiki_tool`.\"\"\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "])\n",
        "tech_tools = [tech_rag_tool, wiki_tool] # Provide both tools\n",
        "tech_agent = create_tool_calling_agent(llm, tech_tools, tech_agent_prompt)\n",
        "tech_agent_executor = AgentExecutor(agent=tech_agent, tools=tech_tools, verbose=False)\n",
        "\n",
        "\n",
        "class TriageDecision(BaseModel):\n",
        "    agent: Literal[\"HR\", \"Technical\"]\n",
        "triage_prompt = ChatPromptTemplate.from_messages([(\"system\", \"Is this query for 'HR' or 'Technical'? Respond only with the structured output.\"), (\"human\", \"Query: {query}\")])\n",
        "triage_agent_runnable = triage_prompt | llm.with_structured_output(TriageDecision)\n",
        "\n",
        "\n",
        "# 6. Orchestrate the System (No changes here)\n",
        "# ---------------------------------------------\n",
        "async def run_agent_system(query: str):\n",
        "    print(\"=\"*80)\n",
        "    print(f\"üë§ User Query: {query}\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"üõ°Ô∏è  Running Guardrail Check...\")\n",
        "    guardrail_result = await guardrail_agent_runnable.ainvoke({\"query\": query})\n",
        "    if not guardrail_result.is_valid:\n",
        "        print(f\"‚ùå Guardrail Blocked Input. Reason: {guardrail_result.reasoning}\\n\")\n",
        "        return\n",
        "    print(f\"‚úÖ Guardrail Passed.\")\n",
        "    print(\"\\nüö¶ Running Triage Agent...\")\n",
        "    triage_decision = await triage_agent_runnable.ainvoke({\"query\": query})\n",
        "    selected_agent = triage_decision.agent\n",
        "    print(f\"üéØ Specialist selected: {selected_agent}\")\n",
        "    print(f\"\\n‚ñ∂Ô∏è  Invoking {selected_agent} Agent...\")\n",
        "    if selected_agent == \"HR\":\n",
        "        result = await hr_agent_executor.ainvoke({\"input\": query})\n",
        "    else:\n",
        "        result = await tech_agent_executor.ainvoke({\"input\": query})\n",
        "    final_answer = result.get('output', 'Sorry, I could not process your request.')\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"ü§ñ Final Answer:\")\n",
        "    print(textwrap.fill(final_answer, width=80))\n",
        "    print(\"-\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# 7. Run Example Queries\n",
        "# ----------------------\n",
        "async def main():\n",
        "    # Test 1: An internal tech question that should use `tech_rag_tool`\n",
        "    await run_agent_system(\"Where is the repository for the Phoenix project's frontend?\")\n",
        "\n",
        "    # Test 2: A general tech question that should use `wiki_tool`\n",
        "    await run_agent_system(\"What is FastAPI?\")\n",
        "\n",
        "    # Test 3: An HR question that should use `hr_rag_tool`\n",
        "    await run_agent_system(\"What's the policy on compassionate leave?\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if llm and hr_retriever and tech_retriever:\n",
        "        try:\n",
        "             await main()\n",
        "        except NameError:\n",
        "             asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xb-FwxvNDQ80",
        "outputId": "ac68bdf9-6281-436a-f592-b297afa9bd9f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Gemini API Key configured successfully.\n",
            "\n",
            "‚öôÔ∏è  Setting up HR RAG pipeline...\n",
            "‚úÖ HR RAG pipeline setup complete.\n",
            "\n",
            "‚öôÔ∏è  Setting up Technical RAG pipeline...\n",
            "‚úÖ Technical RAG pipeline setup complete.\n",
            "================================================================================\n",
            "üë§ User Query: Where is the repository for the Phoenix project's frontend?\n",
            "================================================================================\n",
            "üõ°Ô∏è  Running Guardrail Check...\n",
            "‚ùå Guardrail Blocked Input. Reason: Unable to access external websites or specific file systems to check for the Phoenix project's frontend repository.\n",
            "\n",
            "================================================================================\n",
            "üë§ User Query: What is FastAPI?\n",
            "================================================================================\n",
            "üõ°Ô∏è  Running Guardrail Check...\n",
            "‚ùå Guardrail Blocked Input. Reason: The available tools lack the information to answer this question.\n",
            "\n",
            "================================================================================\n",
            "üë§ User Query: What's the policy on compassionate leave?\n",
            "================================================================================\n",
            "üõ°Ô∏è  Running Guardrail Check...\n",
            "‚úÖ Guardrail Passed.\n",
            "\n",
            "üö¶ Running Triage Agent...\n",
            "üéØ Specialist selected: HR\n",
            "\n",
            "‚ñ∂Ô∏è  Invoking HR Agent...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ü§ñ Final Answer:\n",
            "I'm sorry, I cannot answer this question. The available tools lack the required\n",
            "information.\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HR pipeline with Post-Retrieval Re-ranking"
      ],
      "metadata": {
        "id": "DW_YuYopE5th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flashrank -q\n",
        "!pip install --upgrade langchain langchain-community -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmX0IjY8FGbN",
        "outputId": "cc2d061a-9632-4f14-eba8-95c92f8d2744"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/16.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.2/16.5 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.6/16.5 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.3/16.5 MB\u001b[0m \u001b[31m165.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m166.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m166.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/46.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/86.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install necessary libraries\n",
        "# ---------------------------------\n",
        "# !pip install langchain langchain-google-genai pydantic\n",
        "# !pip install langchain-community faiss-cpu\n",
        "# !pip install flashrank # Add this for the re-ranker\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "import textwrap\n",
        "from typing import List\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# --- RAG Specific Imports ---\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.retrievers.document_compressors.flashrank_rerank import FlashrankRerank\n",
        "\n",
        "# --- SETUP: HR POLICY FILE ---\n",
        "hr_policy_content = \"\"\"\n",
        "# Company Leave Policy\n",
        "\n",
        "## Annual Leave\n",
        "Full-time staff are entitled to 24 days of annual leave per year. This accrues at a rate of 2 days per month. Up to 5 unused days may be carried forward into the next year with manager approval. Requests for leave must be submitted through the HR portal at least two weeks in advance.\n",
        "\n",
        "## Sick Leave\n",
        "Sick leave is for personal illness or injury. It is determined by service duration.\n",
        "- Up to 1 year of service: 2 weeks full pay, 2 weeks half pay.\n",
        "- 1-3 years of service: 4 weeks full pay, 4 weeks half pay.\n",
        "- Over 3 years of service: 8 weeks full pay, 8 weeks half pay.\n",
        "A doctor's note is required for absences longer than 3 consecutive days.\n",
        "\n",
        "## Compassionate Leave\n",
        "Special leave for bereavement is available. Employees can take up to 5 days of paid compassionate leave per year for the loss of a close family member (spouse, child, parent, sibling).\n",
        "\n",
        "## Public Holidays\n",
        "The company observes all official public holidays in the country of employment. These days are paid leave and do not count against an employee's annual leave balance.\n",
        "\n",
        "## Work From Home (WFH) Policy\n",
        "Employees may work from home up to 2 days per week with manager approval. WFH arrangements are based on job role and performance. The company provides a stipend for home office setup. All company IT and security policies must be adhered to while working remotely.\n",
        "\"\"\"\n",
        "with open(\"hr_policy.txt\", \"w\") as f:\n",
        "    f.write(hr_policy_content)\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "# 2. Configure API Key\n",
        "# --------------------\n",
        "llm = None\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n",
        "    llm.invoke(\"Test query\")\n",
        "    print(\"‚úÖ Gemini API Key configured successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error configuring Gemini API: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# 3. Setup RAG Pipeline\n",
        "# ---------------------\n",
        "retriever = None\n",
        "try:\n",
        "    print(\"\\n‚öôÔ∏è  Setting up HR RAG pipeline...\")\n",
        "    loader = TextLoader(\"./hr_policy.txt\")\n",
        "    docs = loader.load()\n",
        "    parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
        "    child_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n",
        "    parent_documents = parent_splitter.split_documents(docs)\n",
        "    child_documents = []\n",
        "    for parent_doc in parent_documents:\n",
        "        child_docs = child_splitter.split_text(parent_doc.page_content)\n",
        "        for child_doc_text in child_docs:\n",
        "            child_documents.append(\n",
        "                Document(page_content=child_doc_text, metadata={\"parent_content\": parent_doc.page_content})\n",
        "            )\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "    vectorstore = FAISS.from_documents(child_documents, embeddings)\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5}) # Retrieve more docs initially for re-ranking\n",
        "    print(\"‚úÖ RAG pipeline setup complete.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error setting up RAG pipeline: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# 4. Instantiate the Re-ranker\n",
        "# ----------------------------\n",
        "print(\"\\n‚ú® Initializing Post-Retrieval Re-ranker...\")\n",
        "reranker = FlashrankRerank()\n",
        "print(\"‚úÖ Re-ranker initialized.\")\n",
        "\n",
        "\n",
        "# 5. Define the HR Tool with Re-ranking\n",
        "# ---------------------------------------\n",
        "@tool\n",
        "def hr_rag_tool(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Searches the HR knowledge base for company policies to answer HR-related questions.\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîé Retrieving initial documents for: '{query}'...\")\n",
        "    # 1. Retrieve initial set of documents. We retrieve more (k=5) to give the re-ranker more to work with.\n",
        "    child_docs = retriever.invoke(query)\n",
        "\n",
        "    # 2. NEW STEP: Re-rank the retrieved documents for relevance.\n",
        "    print(f\"‚ú® Re-ranking {len(child_docs)} retrieved documents...\")\n",
        "    reranked_docs = reranker.compress_documents(documents=child_docs, query=query)\n",
        "    print(f\"‚úÖ Top {len(reranked_docs)} documents after re-ranking:\")\n",
        "\n",
        "    # 3. Extract parent content from the top-ranked documents\n",
        "    unique_parent_contents = {doc.metadata['parent_content'] for doc in reranked_docs}\n",
        "\n",
        "    # 4. Format the final context\n",
        "    context = \"\\n\\n\".join(unique_parent_contents)\n",
        "    return f\"Retrieved and re-ranked context:\\n{context}\"\n",
        "\n",
        "\n",
        "# 6. Define the HR Agent\n",
        "# ----------------------\n",
        "hr_agent_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful H.R. assistant. You must use the `hr_rag_tool` to find information in the company policy document. Answer the user's question based ONLY on the final context provided by the tool.\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "])\n",
        "hr_tools = [hr_rag_tool]\n",
        "hr_agent = create_tool_calling_agent(llm, hr_tools, hr_agent_prompt)\n",
        "hr_agent_executor = AgentExecutor(agent=hr_agent, tools=hr_tools, verbose=False)\n",
        "\n",
        "\n",
        "# 7. Run the System\n",
        "# -----------------\n",
        "async def run_hr_system(query: str):\n",
        "    print(\"=\"*80)\n",
        "    print(f\"üë§ User Query: {query}\")\n",
        "    print(\"=\"*80)\n",
        "    result = await hr_agent_executor.ainvoke({\"input\": query})\n",
        "    final_answer = result.get('output', 'Sorry, I could not process your request.')\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"ü§ñ Final Answer:\")\n",
        "    print(textwrap.fill(final_answer, width=80))\n",
        "    print(\"-\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "async def main():\n",
        "    # A query where re-ranking can help distinguish between different types of leave\n",
        "    await run_hr_system(\"I was sick for a week and my manager says I should have submitted a request in advance. Is that right?\")\n",
        "    # A query about getting money for WFH setup\n",
        "    await run_hr_system(\"Do I get any money to set up my home office for remote work?\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if llm and retriever:\n",
        "        try:\n",
        "            asyncio.run(main())\n",
        "        except RuntimeError:\n",
        "            # This handles running in a notebook where an event loop is already running.\n",
        "            await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1ebV1O1EFJY",
        "outputId": "0ef79809-c5c0-474a-f650-21a9c26b11f3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Gemini API Key configured successfully.\n",
            "\n",
            "‚öôÔ∏è  Setting up HR RAG pipeline...\n",
            "‚úÖ RAG pipeline setup complete.\n",
            "\n",
            "‚ú® Initializing Post-Retrieval Re-ranker...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ms-marco-MultiBERT-L-12.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 98.7M/98.7M [00:00<00:00, 145MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Re-ranker initialized.\n",
            "================================================================================\n",
            "üë§ User Query: I was sick for a week and my manager says I should have submitted a request in advance. Is that right?\n",
            "================================================================================\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ü§ñ Final Answer:\n",
            "I am sorry, I cannot answer this question. I do not have access to the company's\n",
            "policy on this matter.  I need more information to assist you.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "üë§ User Query: Do I get any money to set up my home office for remote work?\n",
            "================================================================================\n",
            "\n",
            "üîé Retrieving initial documents for: 'Home office stipend policy'...\n",
            "‚ú® Re-ranking 4 retrieved documents...\n",
            "‚úÖ Top 3 documents after re-ranking:\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ü§ñ Final Answer:\n",
            "Based on the company policy, yes, there is a stipend provided for home office\n",
            "setup.  However, the exact amount isn't specified here.  You should contact HR\n",
            "for details on the stipend amount.\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-20271066.py:163: RuntimeWarning: coroutine 'main' was never awaited\n",
            "  await main()\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post-Retrieval Re-ranking"
      ],
      "metadata": {
        "id": "Uvad_6RIFq9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install necessary libraries\n",
        "# ---------------------------------\n",
        "# !pip install langchain langchain-google-genai wikipedia pydantic\n",
        "# !pip install langchain-community faiss-cpu\n",
        "# !pip install flashrank\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "import textwrap\n",
        "from typing import Literal, List\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.documents import Document\n",
        "import wikipedia\n",
        "\n",
        "# --- RAG Specific Imports ---\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.retrievers.document_compressors.flashrank_rerank import FlashrankRerank\n",
        "\n",
        "# --- SETUP: Create Dummy Knowledge Base Files ---\n",
        "\n",
        "# HR Policy Document\n",
        "hr_policy_content = \"\"\"\n",
        "# Company Leave Policy\n",
        "\n",
        "## Annual Leave\n",
        "Full-time staff are entitled to 24 days of annual leave per year. This accrues at a rate of 2 days per month. Up to 5 unused days may be carried forward into the next year with manager approval. Requests for leave must be submitted through the HR portal at least two weeks in advance.\n",
        "\n",
        "## Sick Leave\n",
        "Sick leave is for personal illness or injury. It is determined by service duration. A doctor's note is required for absences longer than 3 consecutive days. After 1 year of service, employees get 4 weeks full pay.\n",
        "\n",
        "## Compassionate Leave\n",
        "Employees can take up to 5 days of paid compassionate leave per year for the loss of a close family member.\n",
        "\n",
        "## Work From Home (WFH) Policy\n",
        "Employees may work from home up to 2 days per week. The company provides a stipend for home office setup. All company IT and security policies must be adhered to while working remotely.\n",
        "\"\"\"\n",
        "with open(\"hr_policy.txt\", \"w\") as f:\n",
        "    f.write(hr_policy_content)\n",
        "\n",
        "# Technical Knowledge Base Document\n",
        "tech_docs_content = \"\"\"\n",
        "# Internal Technical Documentation\n",
        "\n",
        "## Project Phoenix: Frontend\n",
        "- Repository: git.corp.example.com/phoenix/frontend-app\n",
        "- Language: TypeScript, Framework: React\n",
        "- Description: This is the main customer-facing web application.\n",
        "\n",
        "## Project Phoenix: Backend\n",
        "- Repository: git.corp.example.com/phoenix/backend-services\n",
        "- Language: Python, Framework: FastAPI\n",
        "- Description: These are the microservices that power the Phoenix frontend.\n",
        "\n",
        "## Deployment Procedures\n",
        "- Deployments to production must go through the CI/CD pipeline in Jenkins.\n",
        "- Request a production deployment by creating a JIRA ticket with the 'DEVOPS' component.\n",
        "\"\"\"\n",
        "with open(\"tech_docs.txt\", \"w\") as f:\n",
        "    f.write(tech_docs_content)\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "# 2. Configure API Key\n",
        "# --------------------\n",
        "llm = None\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n",
        "    llm.invoke(\"Test query\")\n",
        "    print(\"‚úÖ Gemini API Key configured successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error configuring Gemini API: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# 3. Setup RAG Pipelines\n",
        "# ----------------------\n",
        "hr_retriever = None\n",
        "tech_retriever = None\n",
        "\n",
        "# Helper function to create a RAG retriever\n",
        "def create_rag_retriever(file_path: str) -> FAISS.as_retriever:\n",
        "    loader = TextLoader(file_path)\n",
        "    docs = loader.load()\n",
        "    parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=150)\n",
        "    child_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n",
        "    parent_documents = parent_splitter.split_documents(docs)\n",
        "    child_documents = []\n",
        "    for parent_doc in parent_documents:\n",
        "        child_docs = child_splitter.split_text(parent_doc.page_content)\n",
        "        for child_doc_text in child_docs:\n",
        "            child_documents.append(\n",
        "                Document(page_content=child_doc_text, metadata={\"parent_content\": parent_doc.page_content})\n",
        "            )\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "    vectorstore = FAISS.from_documents(child_documents, embeddings)\n",
        "    # Retrieve more documents initially to feed into the re-ranker\n",
        "    return vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "try:\n",
        "    print(\"\\n‚öôÔ∏è  Setting up HR RAG pipeline...\")\n",
        "    hr_retriever = create_rag_retriever(\"./hr_policy.txt\")\n",
        "    print(\"‚úÖ HR RAG pipeline setup complete.\")\n",
        "\n",
        "    print(\"\\n‚öôÔ∏è  Setting up Technical RAG pipeline...\")\n",
        "    tech_retriever = create_rag_retriever(\"./tech_docs.txt\")\n",
        "    print(\"‚úÖ Technical RAG pipeline setup complete.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error setting up RAG pipelines: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# 4. Instantiate the Re-ranker\n",
        "# ----------------------------\n",
        "print(\"\\n‚ú® Initializing Post-Retrieval Re-ranker...\")\n",
        "reranker = FlashrankRerank()\n",
        "print(\"‚úÖ Re-ranker initialized.\")\n",
        "\n",
        "\n",
        "# 5. Define All Tools\n",
        "# -------------------\n",
        "@tool\n",
        "def hr_rag_tool(query: str) -> str:\n",
        "    \"\"\"Searches the HR knowledge base for company policies. Use for HR-related questions.\"\"\"\n",
        "    print(f\"\\nüîé Retrieving initial HR documents for: '{query}'...\")\n",
        "    child_docs = hr_retriever.invoke(query)\n",
        "    print(f\"‚ú® Re-ranking {len(child_docs)} retrieved HR documents...\")\n",
        "    reranked_docs = reranker.compress_documents(documents=child_docs, query=query)\n",
        "    unique_parent_contents = {doc.metadata['parent_content'] for doc in reranked_docs}\n",
        "    return \"Retrieved and re-ranked context:\\n\" + \"\\n\\n\".join(unique_parent_contents)\n",
        "\n",
        "@tool\n",
        "def tech_rag_tool(query: str) -> str:\n",
        "    \"\"\"Searches internal tech docs for company projects, repos, and standards.\"\"\"\n",
        "    print(f\"\\nüîé Retrieving internal tech context for: '{query}'...\")\n",
        "    docs = tech_retriever.invoke(query)\n",
        "    context = \"\\n\\n\".join(doc.metadata['parent_content'] for doc in docs)\n",
        "    return f\"Retrieved context:\\n{context}\"\n",
        "\n",
        "@tool\n",
        "def wiki_tool(query: str) -> str:\n",
        "    \"\"\"Fetches a summary from Wikipedia for general knowledge technical questions.\"\"\"\n",
        "    print(f\"\\nüîé Searching Wikipedia for: '{query}'...\")\n",
        "    try:\n",
        "        return wikipedia.summary(query, auto_suggest=False, sentences=5)\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred fetching from Wikipedia: {e}\"\n",
        "\n",
        "\n",
        "# 6. Define All Agents\n",
        "# --------------------\n",
        "\n",
        "# Guardrail Agent\n",
        "class GuardrailOutput(BaseModel):\n",
        "    is_valid: bool; reasoning: str\n",
        "guardrail_prompt = ChatPromptTemplate.from_messages([(\"system\", \"Is the user's query about HR or Technical topics? Respond only with the structured output.\"), (\"human\", \"Query: {query}\")])\n",
        "guardrail_agent_runnable = guardrail_prompt | llm.with_structured_output(GuardrailOutput)\n",
        "\n",
        "# HR Agent (with advanced RAG tool)\n",
        "hr_agent_prompt = ChatPromptTemplate.from_messages([(\"system\", \"You are an HR assistant. Use the `hr_rag_tool` to answer questions based ONLY on the retrieved context.\"), (\"human\", \"{input}\"), (\"placeholder\", \"{agent_scratchpad}\")])\n",
        "hr_tools = [hr_rag_tool]\n",
        "hr_agent = create_tool_calling_agent(llm, hr_tools, hr_agent_prompt)\n",
        "hr_agent_executor = AgentExecutor(agent=hr_agent, tools=hr_tools, verbose=False)\n",
        "\n",
        "# Technical Agent (with two tools)\n",
        "tech_agent_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a technical assistant with two tools: `tech_rag_tool` for internal company tech info, and `wiki_tool` for public general tech knowledge. You must choose the appropriate tool.\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "])\n",
        "tech_tools = [tech_rag_tool, wiki_tool]\n",
        "tech_agent = create_tool_calling_agent(llm, tech_tools, tech_agent_prompt)\n",
        "tech_agent_executor = AgentExecutor(agent=tech_agent, tools=tech_tools, verbose=False)\n",
        "\n",
        "# Triage Agent\n",
        "class TriageDecision(BaseModel):\n",
        "    agent: Literal[\"HR\", \"Technical\"]\n",
        "triage_prompt = ChatPromptTemplate.from_messages([(\"system\", \"Is this query for 'HR' or 'Technical'? Respond only with the structured output.\"), (\"human\", \"Query: {query}\")])\n",
        "triage_agent_runnable = triage_prompt | llm.with_structured_output(TriageDecision)\n",
        "\n",
        "\n",
        "# 7. Orchestrate the System\n",
        "# -------------------------\n",
        "async def run_agent_system(query: str):\n",
        "    print(\"=\"*80)\n",
        "    print(f\"üë§ User Query: {query}\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"üõ°Ô∏è  Running Guardrail Check...\")\n",
        "    guardrail_result = await guardrail_agent_runnable.ainvoke({\"query\": query})\n",
        "    if not guardrail_result.is_valid:\n",
        "        print(f\"‚ùå Guardrail Blocked Input. Reason: {guardrail_result.reasoning}\\n\")\n",
        "        return\n",
        "    print(f\"‚úÖ Guardrail Passed.\")\n",
        "    print(\"\\nüö¶ Running Triage Agent...\")\n",
        "    triage_decision = await triage_agent_runnable.ainvoke({\"query\": query})\n",
        "    selected_agent = triage_decision.agent\n",
        "    print(f\"üéØ Specialist selected: {selected_agent}\")\n",
        "    print(f\"\\n‚ñ∂Ô∏è  Invoking {selected_agent} Agent...\")\n",
        "    if selected_agent == \"HR\":\n",
        "        result = await hr_agent_executor.ainvoke({\"input\": query})\n",
        "    else:\n",
        "        result = await tech_agent_executor.ainvoke({\"input\": query})\n",
        "    final_answer = result.get('output', 'Sorry, I could not process your request.')\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"ü§ñ Final Answer:\")\n",
        "    print(textwrap.fill(final_answer, width=80))\n",
        "    print(\"-\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# 8. Run Example Queries\n",
        "# ----------------------\n",
        "async def main():\n",
        "    # Test 1: An internal tech question\n",
        "    await run_agent_system(\"Where can I find the backend repo for Project Phoenix?\")\n",
        "    # Test 2: A general tech question\n",
        "    await run_agent_system(\"What is the difference between TypeScript and JavaScript?\")\n",
        "    # Test 3: An HR question\n",
        "    await run_agent_system(\"If I'm sick for more than 3 days, what do I need to do?\")\n",
        "    # Test 4: An out-of-scope question\n",
        "    await run_agent_system(\"What are the best tourist spots in Mumbai?\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if llm and hr_retriever and tech_retriever:\n",
        "        try:\n",
        "            asyncio.run(main())\n",
        "        except RuntimeError:\n",
        "            await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtMpXEXtE7P0",
        "outputId": "c0086550-607b-4284-c462-d9763a0814f0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Gemini API Key configured successfully.\n",
            "\n",
            "‚öôÔ∏è  Setting up HR RAG pipeline...\n",
            "‚úÖ HR RAG pipeline setup complete.\n",
            "\n",
            "‚öôÔ∏è  Setting up Technical RAG pipeline...\n",
            "‚úÖ Technical RAG pipeline setup complete.\n",
            "\n",
            "‚ú® Initializing Post-Retrieval Re-ranker...\n",
            "‚úÖ Re-ranker initialized.\n",
            "================================================================================\n",
            "üë§ User Query: Where can I find the backend repo for Project Phoenix?\n",
            "================================================================================\n",
            "üõ°Ô∏è  Running Guardrail Check...\n",
            "‚ùå Guardrail Blocked Input. Reason: Query is about Technical topics\n",
            "\n",
            "================================================================================\n",
            "üë§ User Query: What is the difference between TypeScript and JavaScript?\n",
            "================================================================================\n",
            "üõ°Ô∏è  Running Guardrail Check...\n",
            "‚úÖ Guardrail Passed.\n",
            "\n",
            "üö¶ Running Triage Agent...\n",
            "üéØ Specialist selected: Technical\n",
            "\n",
            "‚ñ∂Ô∏è  Invoking Technical Agent...\n",
            "\n",
            "üîé Searching Wikipedia for: 'What is the difference between TypeScript and JavaScript?'...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ü§ñ Final Answer:\n",
            "I cannot answer this question using the available tools.  The `wiki_tool` failed\n",
            "to find relevant information.  To get an answer, I would need access to a tool\n",
            "that can access and process information from a reliable source like Wikipedia or\n",
            "a similar knowledge base.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "üë§ User Query: If I'm sick for more than 3 days, what do I need to do?\n",
            "================================================================================\n",
            "üõ°Ô∏è  Running Guardrail Check...\n",
            "‚ùå Guardrail Blocked Input. Reason: This query is related to HR policies and is outside the scope of this tool.\n",
            "\n",
            "================================================================================\n",
            "üë§ User Query: What are the best tourist spots in Mumbai?\n",
            "================================================================================\n",
            "üõ°Ô∏è  Running Guardrail Check...\n",
            "‚ùå Guardrail Blocked Input. Reason: The query is about tourist spots, which is not related to HR or Technical topics.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4172282171.py:237: RuntimeWarning: coroutine 'main' was never awaited\n",
            "  await main()\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final"
      ],
      "metadata": {
        "id": "KjCQVEEIGSZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install all necessary libraries\n",
        "# ------------------------------------\n",
        "# !pip install langchain langchain-google-genai wikipedia pydantic\n",
        "# !pip install langchain-community faiss-cpu\n",
        "# !pip install flashrank\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "import textwrap\n",
        "from typing import Literal, List\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import wikipedia\n",
        "\n",
        "# --- RAG Specific Imports ---\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.retrievers.document_compressors.flashrank_rerank import FlashrankRerank\n",
        "\n",
        "# --- SETUP: Create Dummy Knowledge Base Files ---\n",
        "\n",
        "# HR Policy Document\n",
        "hr_policy_content = \"\"\"\n",
        "# Company Leave Policy\n",
        "\n",
        "## Annual Leave\n",
        "Full-time staff are entitled to 24 days of annual leave per year. This accrues at a rate of 2 days per month. Up to 5 unused days may be carried forward into the next year with manager approval. Requests for leave must be submitted through the HR portal at least two weeks in advance.\n",
        "\n",
        "## Sick Leave\n",
        "Sick leave is for personal illness or injury. It is determined by service duration. A doctor's note is required for absences longer than 3 consecutive days. After 1 year of service, employees get 4 weeks full pay.\n",
        "\n",
        "## Compassionate Leave\n",
        "Employees can take up to 5 days of paid compassionate leave per year for the loss of a close family member.\n",
        "\n",
        "## Work From Home (WFH) Policy\n",
        "Employees may work from home up to 2 days per week. The company provides a stipend for home office setup. All company IT and security policies must be adhered to while working remotely.\n",
        "\"\"\"\n",
        "with open(\"hr_policy.txt\", \"w\") as f:\n",
        "    f.write(hr_policy_content)\n",
        "\n",
        "# Technical Knowledge Base Document\n",
        "tech_docs_content = \"\"\"\n",
        "# Internal Technical Documentation\n",
        "\n",
        "## Project Phoenix: Frontend\n",
        "- Repository: git.corp.example.com/phoenix/frontend-app\n",
        "- Language: TypeScript, Framework: React\n",
        "- Description: This is the main customer-facing web application.\n",
        "\n",
        "## Project Phoenix: Backend\n",
        "- Repository: git.corp.example.com/phoenix/backend-services\n",
        "- Language: Python, Framework: FastAPI\n",
        "- Description: These are the microservices that power the Phoenix frontend.\n",
        "\n",
        "## Deployment Procedures\n",
        "- Deployments to production must go through the CI/CD pipeline in Jenkins.\n",
        "- Request a production deployment by creating a JIRA ticket with the 'DEVOPS' component.\n",
        "\"\"\"\n",
        "with open(\"tech_docs.txt\", \"w\") as f:\n",
        "    f.write(tech_docs_content)\n",
        "# ------------------------------------\n",
        "\n",
        "\n",
        "# 2. Configure API Key\n",
        "# --------------------\n",
        "llm = None\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n",
        "    llm.invoke(\"Test query\")\n",
        "    print(\"‚úÖ Gemini API Key configured successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error configuring Gemini API: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# 3. Setup RAG Pipelines\n",
        "# ----------------------\n",
        "hr_retriever = None\n",
        "tech_retriever = None\n",
        "\n",
        "# Helper function to create a RAG retriever\n",
        "def create_rag_retriever(file_path: str) -> FAISS.as_retriever:\n",
        "    loader = TextLoader(file_path)\n",
        "    docs = loader.load()\n",
        "    parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=150)\n",
        "    child_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n",
        "    parent_documents = parent_splitter.split_documents(docs)\n",
        "    child_documents = []\n",
        "    for parent_doc in parent_documents:\n",
        "        child_docs = child_splitter.split_text(parent_doc.page_content)\n",
        "        for child_doc_text in child_docs:\n",
        "            child_documents.append(\n",
        "                Document(page_content=child_doc_text, metadata={\"parent_content\": parent_doc.page_content})\n",
        "            )\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "    vectorstore = FAISS.from_documents(child_documents, embeddings)\n",
        "    # Retrieve more documents initially (k=5) to feed into the re-ranker\n",
        "    return vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "try:\n",
        "    print(\"\\n‚öôÔ∏è  Setting up HR RAG pipeline...\")\n",
        "    hr_retriever = create_rag_retriever(\"./hr_policy.txt\")\n",
        "    print(\"‚úÖ HR RAG pipeline setup complete.\")\n",
        "\n",
        "    print(\"\\n‚öôÔ∏è  Setting up Technical RAG pipeline...\")\n",
        "    tech_retriever = create_rag_retriever(\"./tech_docs.txt\")\n",
        "    print(\"‚úÖ Technical RAG pipeline setup complete.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error setting up RAG pipelines: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# 4. Instantiate Re-ranker and Query Expansion Chain\n",
        "# --------------------------------------------------\n",
        "print(\"\\n‚ú® Initializing Re-ranker and Query Expansion chain...\")\n",
        "reranker = FlashrankRerank()\n",
        "query_expansion_prompt = ChatPromptTemplate.from_template(\"Rewrite the user query into 3 alternative versions to improve vector search recall.\\n\\nOriginal Query:\\n{query}\\n\\nExpanded Queries:\")\n",
        "query_expansion_chain = query_expansion_prompt | llm | StrOutputParser()\n",
        "print(\"‚úÖ Re-ranker and Query Expansion chain initialized.\")\n",
        "\n",
        "\n",
        "# 5. Define All Tools with Full RAG Capabilities\n",
        "# ----------------------------------------------\n",
        "@tool\n",
        "def hr_rag_tool(query: str) -> str:\n",
        "    \"\"\"Searches the HR knowledge base for company policies. Use for HR-related questions.\"\"\"\n",
        "    print(f\"\\n[HR Tool] ‚û°Ô∏è Query: '{query}'\")\n",
        "    # 1. Pre-retrieval: Query Expansion\n",
        "    expanded_queries_str = query_expansion_chain.invoke({\"query\": query})\n",
        "    all_queries = [query] + expanded_queries_str.strip().split('\\n')\n",
        "    print(f\"[HR Tool] üîç Expanded queries: {all_queries}\")\n",
        "    # 2. Retrieval\n",
        "    all_retrieved_docs = []\n",
        "    for q in all_queries:\n",
        "        all_retrieved_docs.extend(hr_retriever.invoke(q))\n",
        "    # 3. Post-retrieval: Re-ranking\n",
        "    print(f\"[HR Tool] ‚ú® Re-ranking {len(all_retrieved_docs)} documents...\")\n",
        "    reranked_docs = reranker.compress_documents(documents=all_retrieved_docs, query=query)\n",
        "    unique_parent_contents = {doc.metadata['parent_content'] for doc in reranked_docs}\n",
        "    return \"Retrieved and re-ranked context:\\n\" + \"\\n\\n\".join(unique_parent_contents)\n",
        "\n",
        "@tool\n",
        "def tech_rag_tool(query: str) -> str:\n",
        "    \"\"\"Searches internal tech docs for company projects, repos, and standards.\"\"\"\n",
        "    print(f\"\\n[Tech Tool] ‚û°Ô∏è Query: '{query}'\")\n",
        "    # 1. Pre-retrieval: Query Expansion\n",
        "    expanded_queries_str = query_expansion_chain.invoke({\"query\": query})\n",
        "    all_queries = [query] + expanded_queries_str.strip().split('\\n')\n",
        "    print(f\"[Tech Tool] üîç Expanded queries: {all_queries}\")\n",
        "    # 2. Retrieval\n",
        "    all_retrieved_docs = []\n",
        "    for q in all_queries:\n",
        "        all_retrieved_docs.extend(tech_retriever.invoke(q))\n",
        "    # 3. Post-retrieval: Re-ranking\n",
        "    print(f\"[Tech Tool] ‚ú® Re-ranking {len(all_retrieved_docs)} documents...\")\n",
        "    reranked_docs = reranker.compress_documents(documents=all_retrieved_docs, query=query)\n",
        "    unique_parent_contents = {doc.metadata['parent_content'] for doc in reranked_docs}\n",
        "    return f\"Retrieved and re-ranked context:\\n\" + \"\\n\\n\".join(unique_parent_contents)\n",
        "\n",
        "@tool\n",
        "def wiki_tool(query: str) -> str:\n",
        "    \"\"\"Fetches a summary from Wikipedia for general knowledge technical questions.\"\"\"\n",
        "    print(f\"\\n[Wiki Tool] ‚û°Ô∏è Query: '{query}'...\")\n",
        "    try:\n",
        "        return wikipedia.summary(query, auto_suggest=False, sentences=5)\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred fetching from Wikipedia: {e}\"\n",
        "\n",
        "\n",
        "# 6. Define All Agents\n",
        "# --------------------\n",
        "# Guardrail Agent\n",
        "class GuardrailOutput(BaseModel):\n",
        "    is_valid: bool; reasoning: str\n",
        "guardrail_prompt = ChatPromptTemplate.from_messages([(\"system\", \"Is the user's query about HR or Technical topics? Respond only with the structured output.\"), (\"human\", \"Query: {query}\")])\n",
        "guardrail_agent_runnable = guardrail_prompt | llm.with_structured_output(GuardrailOutput)\n",
        "\n",
        "# HR Agent\n",
        "hr_agent_prompt = ChatPromptTemplate.from_messages([(\"system\", \"You are an HR assistant. Use the `hr_rag_tool` to answer questions based ONLY on the retrieved context.\"), (\"human\", \"{input}\"), (\"placeholder\", \"{agent_scratchpad}\")])\n",
        "hr_tools = [hr_rag_tool]\n",
        "hr_agent = create_tool_calling_agent(llm, hr_tools, hr_agent_prompt)\n",
        "hr_agent_executor = AgentExecutor(agent=hr_agent, tools=hr_tools, verbose=False)\n",
        "\n",
        "# Technical Agent\n",
        "tech_agent_prompt = ChatPromptTemplate.from_messages([(\"system\", \"You are a technical assistant with two tools: `tech_rag_tool` for internal company tech info, and `wiki_tool` for public general tech knowledge. You must choose the appropriate tool.\"), (\"human\", \"{input}\"), (\"placeholder\", \"{agent_scratchpad}\")])\n",
        "tech_tools = [tech_rag_tool, wiki_tool]\n",
        "tech_agent = create_tool_calling_agent(llm, tech_tools, tech_agent_prompt)\n",
        "tech_agent_executor = AgentExecutor(agent=tech_agent, tools=tech_tools, verbose=False)\n",
        "\n",
        "# Triage Agent\n",
        "class TriageDecision(BaseModel):\n",
        "    agent: Literal[\"HR\", \"Technical\"]\n",
        "triage_prompt = ChatPromptTemplate.from_messages([(\"system\", \"Is this query for 'HR' or 'Technical'? Respond only with the structured output.\"), (\"human\", \"Query: {query}\")])\n",
        "triage_agent_runnable = triage_prompt | llm.with_structured_output(TriageDecision)\n",
        "\n",
        "\n",
        "# 7. Orchestrate the System\n",
        "# -------------------------\n",
        "async def run_agent_system(query: str):\n",
        "    print(\"=\"*80)\n",
        "    print(f\"üë§ User Query: {query}\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"üõ°Ô∏è  Running Guardrail Check...\")\n",
        "    guardrail_result = await guardrail_agent_runnable.ainvoke({\"query\": query})\n",
        "    if not guardrail_result.is_valid:\n",
        "        print(f\"‚ùå Guardrail Blocked Input. Reason: {guardrail_result.reasoning}\\n\")\n",
        "        return\n",
        "    print(f\"‚úÖ Guardrail Passed.\")\n",
        "    print(\"\\nüö¶ Running Triage Agent...\")\n",
        "    triage_decision = await triage_agent_runnable.ainvoke({\"query\": query})\n",
        "    selected_agent = triage_decision.agent\n",
        "    print(f\"üéØ Specialist selected: {selected_agent}\")\n",
        "    print(f\"\\n‚ñ∂Ô∏è  Invoking {selected_agent} Agent...\")\n",
        "    if selected_agent == \"HR\":\n",
        "        result = await hr_agent_executor.ainvoke({\"input\": query})\n",
        "    else:\n",
        "        result = await tech_agent_executor.ainvoke({\"input\": query})\n",
        "    final_answer = result.get('output', 'Sorry, I could not process your request.')\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"ü§ñ Final Answer:\")\n",
        "    print(textwrap.fill(final_answer, width=80))\n",
        "    print(\"-\"*80 + \"\\n\")\n",
        "\n",
        "\n",
        "# 8. Run Example Queries\n",
        "# ----------------------\n",
        "async def main():\n",
        "    # Test 1: Internal tech question -> Triage to Tech -> Tech Agent uses tech_rag_tool\n",
        "    await run_agent_system(\"Where can I find the backend repo for Project Phoenix?\")\n",
        "\n",
        "    # Test 2: General tech question -> Triage to Tech -> Tech Agent uses wiki_tool\n",
        "    await run_agent_system(\"What is React?\")\n",
        "\n",
        "    # Test 3: HR question -> Triage to HR -> HR Agent uses hr_rag_tool\n",
        "    await run_agent_system(\"Do I get paid for compassionate leave?\")\n",
        "\n",
        "    # Test 4: Out-of-scope question -> Blocked by Guardrail\n",
        "    await run_agent_system(\"What's the weather like today?\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if llm and hr_retriever and tech_retriever:\n",
        "        try:\n",
        "            asyncio.run(main())\n",
        "        except RuntimeError:\n",
        "            await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zsd1mTJOFr2_",
        "outputId": "f3ecee76-84c8-419a-a5d4-62f0be6411dd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Gemini API Key configured successfully.\n",
            "\n",
            "‚öôÔ∏è  Setting up HR RAG pipeline...\n",
            "‚úÖ HR RAG pipeline setup complete.\n",
            "\n",
            "‚öôÔ∏è  Setting up Technical RAG pipeline...\n",
            "‚úÖ Technical RAG pipeline setup complete.\n",
            "\n",
            "‚ú® Initializing Re-ranker and Query Expansion chain...\n",
            "‚úÖ Re-ranker and Query Expansion chain initialized.\n",
            "================================================================================\n",
            "üë§ User Query: Where can I find the backend repo for Project Phoenix?\n",
            "================================================================================\n",
            "üõ°Ô∏è  Running Guardrail Check...\n",
            "‚ùå Guardrail Blocked Input. Reason: Query is about Technical topics\n",
            "\n",
            "================================================================================\n",
            "üë§ User Query: What is React?\n",
            "================================================================================\n",
            "üõ°Ô∏è  Running Guardrail Check...\n",
            "‚ùå Guardrail Blocked Input. Reason: The query is about a technical topic\n",
            "\n",
            "================================================================================\n",
            "üë§ User Query: Do I get paid for compassionate leave?\n",
            "================================================================================\n",
            "üõ°Ô∏è  Running Guardrail Check...\n",
            "‚úÖ Guardrail Passed.\n",
            "\n",
            "üö¶ Running Triage Agent...\n",
            "üéØ Specialist selected: HR\n",
            "\n",
            "‚ñ∂Ô∏è  Invoking HR Agent...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ü§ñ Final Answer:\n",
            "I'm sorry, I cannot answer this question. The available tools lack the\n",
            "information to address your query.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "üë§ User Query: What's the weather like today?\n",
            "================================================================================\n",
            "üõ°Ô∏è  Running Guardrail Check...\n",
            "‚ùå Guardrail Blocked Input. Reason: This query is not related to HR or Technical topics.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1362040401.py:256: RuntimeWarning: coroutine 'main' was never awaited\n",
            "  await main()\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FnPqBU-mGYah"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}